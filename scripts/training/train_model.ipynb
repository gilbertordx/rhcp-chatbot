{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b998415-19eb-48b9-8d0f-6a499241db65",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Add parent directories to path for imports\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path(\u001b[34;43m__file__\u001b[39;49m).parent.parent.parent))\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfigManager\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogger_setup\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setup_training_logger\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Model Training Script for RHCP Chatbot ML Pipeline.\n",
    "\n",
    "Main training script that coordinates data loading, model training, and evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directories to path for imports\n",
    "sys.path.append(str(Path(__file__).parent.parent.parent))\n",
    "\n",
    "from scripts.utils.config_manager import ConfigManager\n",
    "from scripts.utils.logger_setup import setup_training_logger\n",
    "from scripts.utils.model_utils import ModelUtils\n",
    "from scripts.data.load_data import DataLoader\n",
    "from scripts.data.enhance_data import DataEnhancer\n",
    "from scripts.evaluation.evaluate_model import ModelEvaluator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Main class for coordinating the training pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the trainer.\"\"\"\n",
    "        self.config_manager = ConfigManager()\n",
    "        self.training_config = self.config_manager.get_training_config()\n",
    "        self.data_config = self.config_manager.get_data_config()\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = setup_training_logger(self.training_config)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.data_loader = DataLoader(self.config_manager)\n",
    "        self.data_enhancer = DataEnhancer(self.config_manager)\n",
    "        self.evaluator = ModelEvaluator(self.config_manager)\n",
    "        \n",
    "        self.logger.info(\"Model trainer initialized\")\n",
    "    \n",
    "    def run_training_pipeline(self):\n",
    "        \"\"\"Run the complete training pipeline.\"\"\"\n",
    "        self.logger.info(\"=== STARTING TRAINING PIPELINE ===\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Load and validate data\n",
    "            self.logger.info(\"Step 1: Loading training data...\")\n",
    "            df = self.data_loader.load_training_data()\n",
    "            \n",
    "            # Step 2: Enhance data if configured\n",
    "            enhancement_config = self.training_config.get('enhancement', {})\n",
    "            if enhancement_config.get('enable_minority_class_enhancement', False):\n",
    "                self.logger.info(\"Step 2: Enhancing minority classes...\")\n",
    "                df = self.data_enhancer.enhance_minority_classes(df)\n",
    "            else:\n",
    "                self.logger.info(\"Step 2: Skipping data enhancement (disabled in config)\")\n",
    "            \n",
    "            # Step 3: Split data\n",
    "            self.logger.info(\"Step 3: Splitting data into train/test sets...\")\n",
    "            X_train, X_test, y_train, y_test = self._split_data(df)\n",
    "            \n",
    "            # Step 4: Create and train model\n",
    "            self.logger.info(\"Step 4: Creating and training model...\")\n",
    "            pipeline = ModelUtils.create_pipeline(self.training_config)\n",
    "            \n",
    "            # Set random seed for reproducibility\n",
    "            self._set_random_seed()\n",
    "            \n",
    "            # Train the model\n",
    "            trained_pipeline = ModelUtils.train_model(\n",
    "                pipeline, X_train, y_train, self.training_config, self.logger\n",
    "            )\n",
    "            \n",
    "            # Step 5: Evaluate model\n",
    "            self.logger.info(\"Step 5: Evaluating model performance...\")\n",
    "            \n",
    "            # Test set evaluation\n",
    "            test_results = ModelUtils.evaluate_model(\n",
    "                trained_pipeline, X_test, y_test, self.training_config, self.logger\n",
    "            )\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_results = ModelUtils.cross_validate_model(\n",
    "                trained_pipeline, df['text'], df['intent'], self.training_config, self.logger\n",
    "            )\n",
    "            \n",
    "            # Test on specific cases\n",
    "            test_cases = self.training_config.get('evaluation', {}).get('test_cases', [])\n",
    "            prediction_results = ModelUtils.test_model_predictions(\n",
    "                trained_pipeline, test_cases, self.logger\n",
    "            )\n",
    "            \n",
    "            # Step 6: Save model and results\n",
    "            self.logger.info(\"Step 6: Saving model and results...\")\n",
    "            self._save_training_artifacts(\n",
    "                trained_pipeline, test_results, cv_results, prediction_results, df\n",
    "            )\n",
    "            \n",
    "            # Step 7: Generate comprehensive evaluation\n",
    "            self.logger.info(\"Step 7: Generating evaluation report...\")\n",
    "            self.evaluator.generate_evaluation_report(\n",
    "                trained_pipeline, X_test, y_test, test_results, cv_results\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\"=== TRAINING PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "            return trained_pipeline, test_results, cv_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Training pipeline failed: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _split_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Split data into training and test sets.\"\"\"\n",
    "        training_config = self.training_config['training']\n",
    "        \n",
    "        X = df['text']\n",
    "        y = df['intent']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=training_config['test_size'],\n",
    "            random_state=training_config['random_state'],\n",
    "            stratify=y if training_config['stratify'] else None\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Data split: {len(X_train)} train, {len(X_test)} test samples\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def _set_random_seed(self):\n",
    "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "        reproducibility_config = self.training_config.get('reproducibility', {})\n",
    "        \n",
    "        if reproducibility_config.get('set_global_seed', True):\n",
    "            seed = reproducibility_config.get('seed', 42)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            # Set other random seeds if needed\n",
    "            import random\n",
    "            random.seed(seed)\n",
    "            \n",
    "            self.logger.info(f\"Random seed set to: {seed}\")\n",
    "    \n",
    "    def _save_training_artifacts(self, pipeline, test_results, cv_results, \n",
    "                               prediction_results, df):\n",
    "        \"\"\"Save all training artifacts.\"\"\"\n",
    "        output_config = self.training_config['output']\n",
    "        models_path = self.data_config['paths']['models']\n",
    "        results_path = self.data_config['paths']['results']\n",
    "        \n",
    "        # Create paths\n",
    "        Path(models_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Model file path\n",
    "        model_path = Path(models_path) / output_config['model_filename']\n",
    "        \n",
    "        # Create comprehensive metadata\n",
    "        training_info = {\n",
    "            'total_samples': len(df),\n",
    "            'training_samples': test_results['n_test_samples'] * 4,  # Approximate\n",
    "            'test_samples': test_results['n_test_samples'],\n",
    "            'version': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'enhancement_applied': self.training_config.get('enhancement', {}).get('enable_minority_class_enhancement', False)\n",
    "        }\n",
    "        \n",
    "        metadata = ModelUtils.create_model_metadata(\n",
    "            pipeline, test_results, self.training_config, training_info\n",
    "        )\n",
    "        \n",
    "        # Add cross-validation results to metadata\n",
    "        metadata['cross_validation'] = cv_results\n",
    "        metadata['test_predictions'] = prediction_results\n",
    "        \n",
    "        # Save model with metadata\n",
    "        ModelUtils.save_model(\n",
    "            pipeline, str(model_path), metadata, self.training_config, self.logger\n",
    "        )\n",
    "        \n",
    "        # Save detailed results\n",
    "        results_data = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_path': str(model_path),\n",
    "            'test_evaluation': test_results,\n",
    "            'cross_validation': cv_results,\n",
    "            'test_predictions': prediction_results,\n",
    "            'data_summary': {\n",
    "                'total_samples': len(df),\n",
    "                'unique_intents': df['intent'].nunique(),\n",
    "                'class_distribution': df['intent'].value_counts().to_dict()\n",
    "            },\n",
    "            'configuration': self.training_config\n",
    "        }\n",
    "        \n",
    "        results_file = Path(results_path) / output_config['results_filename']\n",
    "        with open(results_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        self.logger.info(f\"Training artifacts saved:\")\n",
    "        self.logger.info(f\"  Model: {model_path}\")\n",
    "        self.logger.info(f\"  Results: {results_file}\")\n",
    "        \n",
    "        # Create backup if configured\n",
    "        if output_config.get('create_backup', True):\n",
    "            training_files = self.training_config['data']['training_files']\n",
    "            backup_dir = self.data_loader.create_data_backup()\n",
    "            self.logger.info(f\"  Backup: {backup_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for standalone execution.\"\"\"\n",
    "    print(\"üöÄ RHCP Chatbot Model Training Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Initialize and run trainer\n",
    "        trainer = ModelTrainer()\n",
    "        pipeline, test_results, cv_results = trainer.run_training_pipeline()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(f\"\\nüìä PERFORMANCE SUMMARY:\")\n",
    "        print(f\"  Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        print(f\"  Test Macro F1: {test_results['macro_f1']:.4f}\")\n",
    "        print(f\"  CV Accuracy: {cv_results['accuracy']['mean']:.4f} ¬± {cv_results['accuracy']['std']:.4f}\")\n",
    "        print(f\"  CV Macro F1: {cv_results['macro_f1']['mean']:.4f} ¬± {cv_results['macro_f1']['std']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Model is ready for deployment!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå TRAINING FAILED: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbde58c-a49c-4d1a-bf1f-25cf4467557e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
