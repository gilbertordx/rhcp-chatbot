{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b998415-19eb-48b9-8d0f-6a499241db65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/gilberto/Documents/rhcp-chatbot\n",
      "Python path updated to include: /home/gilberto/Documents/rhcp-chatbot\n",
      "Working directory changed to: /home/gilberto/Documents/rhcp-chatbot\n",
      "Config directory exists: True\n",
      "RHCP Chatbot Model Training Pipeline\n",
      "==================================================\n",
      "2025-07-28 21:34:29,108 - training - INFO - Model trainer initialized\n",
      "2025-07-28 21:34:29,110 - training - INFO - === STARTING TRAINING PIPELINE ===\n",
      "2025-07-28 21:34:29,111 - training - INFO - Step 1: Loading training data...\n",
      "2025-07-28 21:34:29,114 - data_processing - INFO - Starting data loading process...\n",
      "2025-07-28 21:34:29,115 - data_processing - INFO - Loading data from 2 files: ['data/processed/base-corpus.json', 'data/processed/rhcp-corpus.json']\n",
      "2025-07-28 21:34:29,120 - data_processing - INFO - Loaded 1041 samples with 70 unique intents\n",
      "2025-07-28 21:34:29,124 - data_processing - INFO - Class balance analysis:\n",
      "2025-07-28 21:34:29,125 - data_processing - INFO -   Most common class: agent.chatbot (72 samples)\n",
      "2025-07-28 21:34:29,126 - data_processing - INFO -   Least common class: greetings.nicetomeetyou (5 samples)\n",
      "2025-07-28 21:34:29,127 - data_processing - INFO -   Imbalance ratio: 14.40:1\n",
      "2025-07-28 21:34:29,129 - training - INFO - Step 2: Enhancing minority classes...\n",
      "2025-07-28 21:34:29,131 - data_processing - INFO - Starting minority class enhancement...\n",
      "2025-07-28 21:34:29,135 - data_processing - INFO - Found 65 minority classes: ['agent.acquaintance', 'intent.outofscope', 'agent.right', 'greetings.hello', 'agent.hobby', 'agent.hungry', 'agent.beautiful', 'agent.ready', 'agent.sure', 'agent.occupation', 'greetings.nicetotalktoyou', 'greetings.nicetoseeyou', 'user.needsadvice', 'dialog.hug', 'appraisal.good', 'appraisal.bad', 'agent.happy', 'dialog.idontcare', 'dialog.sorry', 'appraisal.noproblem', 'appraisal.welcome', 'user.bored', 'user.cannotsleep', 'user.likeagent', 'user.excited', 'dialog.holdon', 'user.lovesagent', 'song.specific', 'album.specific', 'agent.bad', 'agent.beclever', 'user.angry', 'agent.annoying', 'agent.busy', 'agent.marryuser', 'agent.myfriend', 'agent.canyouhelp', 'album.info', 'song.info', 'user.back', 'band.history', 'agent.fire', 'agent.residence', 'band.tours', 'agent.boring', 'agent.boss', 'agent.crazy', 'agent.funny', 'agent.origin', 'band.collaborations', 'band.awards', 'band.influence', 'band.style', 'agent.talktome', 'agent.there', 'agent.clever', 'agent.birthday', 'appraisal.thankyou', 'greetings.howareyou', 'appraisal.welldone', 'agent.good', 'agent.real', 'agent.age', 'song.lyrics', 'greetings.nicetomeetyou']\n",
      "2025-07-28 21:34:29,137 - data_processing - INFO - Enhancing 'agent.acquaintance' with 8 additional examples\n",
      "2025-07-28 21:34:29,138 - data_processing - WARNING - No enhancement templates found for intent: intent.outofscope\n",
      "2025-07-28 21:34:29,139 - data_processing - INFO - Enhancing 'agent.right' with 5 additional examples\n",
      "2025-07-28 21:34:29,142 - data_processing - WARNING - No enhancement templates found for intent: greetings.hello\n",
      "2025-07-28 21:34:29,144 - data_processing - INFO - Enhancing 'agent.hobby' with 5 additional examples\n",
      "2025-07-28 21:34:29,145 - data_processing - INFO - Enhancing 'agent.hungry' with 5 additional examples\n",
      "2025-07-28 21:34:29,146 - data_processing - INFO - Enhancing 'agent.beautiful' with 7 additional examples\n",
      "2025-07-28 21:34:29,148 - data_processing - INFO - Enhancing 'agent.ready' with 5 additional examples\n",
      "2025-07-28 21:34:29,150 - data_processing - INFO - Enhancing 'agent.sure' with 5 additional examples\n",
      "2025-07-28 21:34:29,151 - data_processing - INFO - Enhancing 'agent.occupation' with 5 additional examples\n",
      "2025-07-28 21:34:29,152 - data_processing - WARNING - No enhancement templates found for intent: greetings.nicetotalktoyou\n",
      "2025-07-28 21:34:29,153 - data_processing - WARNING - No enhancement templates found for intent: greetings.nicetoseeyou\n",
      "2025-07-28 21:34:29,154 - data_processing - WARNING - No enhancement templates found for intent: user.needsadvice\n",
      "2025-07-28 21:34:29,155 - data_processing - WARNING - No enhancement templates found for intent: dialog.hug\n",
      "2025-07-28 21:34:29,156 - data_processing - WARNING - No enhancement templates found for intent: appraisal.good\n",
      "2025-07-28 21:34:29,157 - data_processing - WARNING - No enhancement templates found for intent: appraisal.bad\n",
      "2025-07-28 21:34:29,159 - data_processing - WARNING - No enhancement templates found for intent: agent.happy\n",
      "2025-07-28 21:34:29,160 - data_processing - WARNING - No enhancement templates found for intent: dialog.idontcare\n",
      "2025-07-28 21:34:29,161 - data_processing - WARNING - No enhancement templates found for intent: dialog.sorry\n",
      "2025-07-28 21:34:29,163 - data_processing - WARNING - No enhancement templates found for intent: appraisal.noproblem\n",
      "2025-07-28 21:34:29,164 - data_processing - WARNING - No enhancement templates found for intent: appraisal.welcome\n",
      "2025-07-28 21:34:29,165 - data_processing - WARNING - No enhancement templates found for intent: user.bored\n",
      "2025-07-28 21:34:29,167 - data_processing - WARNING - No enhancement templates found for intent: user.cannotsleep\n",
      "2025-07-28 21:34:29,168 - data_processing - WARNING - No enhancement templates found for intent: user.likeagent\n",
      "2025-07-28 21:34:29,169 - data_processing - WARNING - No enhancement templates found for intent: user.excited\n",
      "2025-07-28 21:34:29,171 - data_processing - WARNING - No enhancement templates found for intent: dialog.holdon\n",
      "2025-07-28 21:34:29,172 - data_processing - WARNING - No enhancement templates found for intent: user.lovesagent\n",
      "2025-07-28 21:34:29,174 - data_processing - WARNING - No enhancement templates found for intent: song.specific\n",
      "2025-07-28 21:34:29,175 - data_processing - WARNING - No enhancement templates found for intent: album.specific\n",
      "2025-07-28 21:34:29,176 - data_processing - INFO - Enhancing 'agent.bad' with 7 additional examples\n",
      "2025-07-28 21:34:29,177 - data_processing - INFO - Enhancing 'agent.beclever' with 7 additional examples\n",
      "2025-07-28 21:34:29,178 - data_processing - INFO - Enhancing 'user.angry' with 7 additional examples\n",
      "2025-07-28 21:34:29,179 - data_processing - INFO - Enhancing 'agent.annoying' with 7 additional examples\n",
      "2025-07-28 21:34:29,180 - data_processing - INFO - Enhancing 'agent.busy' with 5 additional examples\n",
      "2025-07-28 21:34:29,181 - data_processing - INFO - Enhancing 'agent.marryuser' with 5 additional examples\n",
      "2025-07-28 21:34:29,182 - data_processing - INFO - Enhancing 'agent.myfriend' with 5 additional examples\n",
      "2025-07-28 21:34:29,183 - data_processing - INFO - Enhancing 'agent.canyouhelp' with 5 additional examples\n",
      "2025-07-28 21:34:29,184 - data_processing - WARNING - No enhancement templates found for intent: album.info\n",
      "2025-07-28 21:34:29,185 - data_processing - WARNING - No enhancement templates found for intent: song.info\n",
      "2025-07-28 21:34:29,186 - data_processing - INFO - Enhancing 'user.back' with 7 additional examples\n",
      "2025-07-28 21:34:29,187 - data_processing - WARNING - No enhancement templates found for intent: band.history\n",
      "2025-07-28 21:34:29,188 - data_processing - INFO - Enhancing 'agent.fire' with 4 additional examples\n",
      "2025-07-28 21:34:29,190 - data_processing - INFO - Enhancing 'agent.residence' with 5 additional examples\n",
      "2025-07-28 21:34:29,191 - data_processing - WARNING - No enhancement templates found for intent: band.tours\n",
      "2025-07-28 21:34:29,192 - data_processing - INFO - Enhancing 'agent.boring' with 5 additional examples\n",
      "2025-07-28 21:34:29,193 - data_processing - INFO - Enhancing 'agent.boss' with 5 additional examples\n",
      "2025-07-28 21:34:29,194 - data_processing - INFO - Enhancing 'agent.crazy' with 5 additional examples\n",
      "2025-07-28 21:34:29,196 - data_processing - INFO - Enhancing 'agent.funny' with 5 additional examples\n",
      "2025-07-28 21:34:29,197 - data_processing - INFO - Enhancing 'agent.origin' with 5 additional examples\n",
      "2025-07-28 21:34:29,198 - data_processing - WARNING - No enhancement templates found for intent: band.collaborations\n",
      "2025-07-28 21:34:29,199 - data_processing - WARNING - No enhancement templates found for intent: band.awards\n",
      "2025-07-28 21:34:29,200 - data_processing - WARNING - No enhancement templates found for intent: band.influence\n",
      "2025-07-28 21:34:29,201 - data_processing - WARNING - No enhancement templates found for intent: band.style\n",
      "2025-07-28 21:34:29,202 - data_processing - INFO - Enhancing 'agent.talktome' with 5 additional examples\n",
      "2025-07-28 21:34:29,203 - data_processing - INFO - Enhancing 'agent.there' with 5 additional examples\n",
      "2025-07-28 21:34:29,203 - data_processing - WARNING - No enhancement templates found for intent: agent.clever\n",
      "2025-07-28 21:34:29,204 - data_processing - INFO - Enhancing 'agent.birthday' with 5 additional examples\n",
      "2025-07-28 21:34:29,205 - data_processing - WARNING - No enhancement templates found for intent: appraisal.thankyou\n",
      "2025-07-28 21:34:29,206 - data_processing - WARNING - No enhancement templates found for intent: greetings.howareyou\n",
      "2025-07-28 21:34:29,207 - data_processing - WARNING - No enhancement templates found for intent: appraisal.welldone\n",
      "2025-07-28 21:34:29,208 - data_processing - WARNING - No enhancement templates found for intent: agent.good\n",
      "2025-07-28 21:34:29,209 - data_processing - WARNING - No enhancement templates found for intent: agent.real\n",
      "2025-07-28 21:34:29,210 - data_processing - WARNING - No enhancement templates found for intent: agent.age\n",
      "2025-07-28 21:34:29,211 - data_processing - WARNING - No enhancement templates found for intent: song.lyrics\n",
      "2025-07-28 21:34:29,212 - data_processing - WARNING - No enhancement templates found for intent: greetings.nicetomeetyou\n",
      "2025-07-28 21:34:29,215 - data_processing - INFO - Added 149 new samples\n",
      "2025-07-28 21:34:29,216 - data_processing - INFO - Total samples: 1041 -> 1190\n",
      "2025-07-28 21:34:29,218 - data_processing - INFO - Updating corpus files with enhanced data...\n",
      "2025-07-28 21:34:29,223 - data_processing - INFO - Updated corpus file: data/processed/base-corpus.json\n",
      "2025-07-28 21:34:29,224 - training - INFO - Step 3: Splitting data into train/test sets...\n",
      "2025-07-28 21:34:29,230 - training - INFO - Data split: 952 train, 238 test samples\n",
      "2025-07-28 21:34:29,231 - training - INFO - Step 4: Creating and training model...\n",
      "2025-07-28 21:34:29,232 - training - INFO - Random seed set to: 42\n",
      "2025-07-28 21:34:29,233 - training - INFO - Starting model training...\n",
      "2025-07-28 21:34:29,234 - training - INFO - Training samples: 952\n",
      "2025-07-28 21:34:29,235 - training - INFO - Unique classes: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 21:34:30,479 - training - INFO - Model training completed successfully\n",
      "2025-07-28 21:34:30,484 - training - INFO - Step 5: Evaluating model performance...\n",
      "2025-07-28 21:34:30,485 - training - INFO - Starting model evaluation...\n",
      "2025-07-28 21:34:30,618 - training - INFO - Evaluation completed - Accuracy: 0.6387, Macro F1: 0.6250\n",
      "2025-07-28 21:34:30,622 - training - INFO - Starting 5-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 152, in __call__\n",
      "    score = scorer._score(\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 400, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['agent.acquaintance' 'agent.age' 'agent.annoying' 'agent.bad'\n",
      " 'agent.beautiful' 'agent.beclever' 'agent.birthday' 'agent.boring'\n",
      " 'agent.boss' 'agent.busy' 'agent.canyouhelp' 'agent.chatbot'\n",
      " 'agent.clever' 'agent.crazy' 'agent.fire' 'agent.funny' 'agent.good'\n",
      " 'agent.happy' 'agent.hobby' 'agent.hungry' 'agent.marryuser'\n",
      " 'agent.myfriend' 'agent.occupation' 'agent.origin' 'agent.ready'\n",
      " 'agent.real' 'agent.residence' 'agent.right' 'agent.sure'\n",
      " 'agent.talktome' 'agent.there' 'album.info' 'album.specific'\n",
      " 'appraisal.bad' 'appraisal.good' 'appraisal.noproblem'\n",
      " 'appraisal.thankyou' 'appraisal.welcome' 'appraisal.welldone'\n",
      " 'band.awards' 'band.collaborations' 'band.history' 'band.influence'\n",
      " 'band.members' 'band.style' 'band.tours' 'dialog.holdon' 'dialog.hug'\n",
      " 'dialog.idontcare' 'dialog.sorry' 'greetings.bye' 'greetings.hello'\n",
      " 'greetings.howareyou' 'greetings.nicetomeetyou' 'greetings.nicetoseeyou'\n",
      " 'greetings.nicetotalktoyou' 'intent.outofscope' 'member.biography'\n",
      " 'song.info' 'song.lyrics' 'song.specific' 'user.angry' 'user.back'\n",
      " 'user.bored' 'user.busy' 'user.cannotsleep' 'user.excited'\n",
      " 'user.likeagent' 'user.lovesagent' 'user.needsadvice']\n",
      "\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 152, in __call__\n",
      "    score = scorer._score(\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 400, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['agent.acquaintance' 'agent.age' 'agent.annoying' 'agent.bad'\n",
      " 'agent.beautiful' 'agent.beclever' 'agent.birthday' 'agent.boring'\n",
      " 'agent.boss' 'agent.busy' 'agent.canyouhelp' 'agent.chatbot'\n",
      " 'agent.clever' 'agent.crazy' 'agent.fire' 'agent.funny' 'agent.good'\n",
      " 'agent.happy' 'agent.hobby' 'agent.hungry' 'agent.marryuser'\n",
      " 'agent.myfriend' 'agent.occupation' 'agent.origin' 'agent.ready'\n",
      " 'agent.real' 'agent.residence' 'agent.right' 'agent.sure'\n",
      " 'agent.talktome' 'agent.there' 'album.info' 'album.specific'\n",
      " 'appraisal.bad' 'appraisal.good' 'appraisal.noproblem'\n",
      " 'appraisal.thankyou' 'appraisal.welcome' 'appraisal.welldone'\n",
      " 'band.awards' 'band.collaborations' 'band.history' 'band.influence'\n",
      " 'band.members' 'band.style' 'band.tours' 'dialog.holdon' 'dialog.hug'\n",
      " 'dialog.idontcare' 'dialog.sorry' 'greetings.bye' 'greetings.hello'\n",
      " 'greetings.howareyou' 'greetings.nicetomeetyou' 'greetings.nicetoseeyou'\n",
      " 'greetings.nicetotalktoyou' 'intent.outofscope' 'member.biography'\n",
      " 'song.info' 'song.lyrics' 'song.specific' 'user.angry' 'user.back'\n",
      " 'user.bored' 'user.busy' 'user.cannotsleep' 'user.excited'\n",
      " 'user.likeagent' 'user.lovesagent' 'user.needsadvice']\n",
      "\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 152, in __call__\n",
      "    score = scorer._score(\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 400, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['agent.acquaintance' 'agent.age' 'agent.annoying' 'agent.bad'\n",
      " 'agent.beautiful' 'agent.beclever' 'agent.birthday' 'agent.boring'\n",
      " 'agent.boss' 'agent.busy' 'agent.canyouhelp' 'agent.chatbot'\n",
      " 'agent.clever' 'agent.crazy' 'agent.fire' 'agent.funny' 'agent.good'\n",
      " 'agent.happy' 'agent.hobby' 'agent.hungry' 'agent.marryuser'\n",
      " 'agent.myfriend' 'agent.occupation' 'agent.origin' 'agent.ready'\n",
      " 'agent.real' 'agent.residence' 'agent.right' 'agent.sure'\n",
      " 'agent.talktome' 'agent.there' 'album.info' 'album.specific'\n",
      " 'appraisal.bad' 'appraisal.good' 'appraisal.noproblem'\n",
      " 'appraisal.thankyou' 'appraisal.welcome' 'appraisal.welldone'\n",
      " 'band.awards' 'band.collaborations' 'band.history' 'band.influence'\n",
      " 'band.members' 'band.style' 'band.tours' 'dialog.holdon' 'dialog.hug'\n",
      " 'dialog.idontcare' 'dialog.sorry' 'greetings.bye' 'greetings.hello'\n",
      " 'greetings.howareyou' 'greetings.nicetomeetyou' 'greetings.nicetoseeyou'\n",
      " 'greetings.nicetotalktoyou' 'intent.outofscope' 'member.biography'\n",
      " 'song.info' 'song.lyrics' 'song.specific' 'user.angry' 'user.back'\n",
      " 'user.bored' 'user.busy' 'user.cannotsleep' 'user.excited'\n",
      " 'user.likeagent' 'user.lovesagent' 'user.needsadvice']\n",
      "\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 152, in __call__\n",
      "    score = scorer._score(\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 400, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['agent.acquaintance' 'agent.age' 'agent.annoying' 'agent.bad'\n",
      " 'agent.beautiful' 'agent.beclever' 'agent.birthday' 'agent.boring'\n",
      " 'agent.boss' 'agent.busy' 'agent.canyouhelp' 'agent.chatbot'\n",
      " 'agent.clever' 'agent.crazy' 'agent.fire' 'agent.funny' 'agent.good'\n",
      " 'agent.happy' 'agent.hobby' 'agent.hungry' 'agent.marryuser'\n",
      " 'agent.myfriend' 'agent.occupation' 'agent.origin' 'agent.ready'\n",
      " 'agent.real' 'agent.residence' 'agent.right' 'agent.sure'\n",
      " 'agent.talktome' 'agent.there' 'album.info' 'album.specific'\n",
      " 'appraisal.bad' 'appraisal.good' 'appraisal.noproblem'\n",
      " 'appraisal.thankyou' 'appraisal.welcome' 'appraisal.welldone'\n",
      " 'band.awards' 'band.collaborations' 'band.history' 'band.influence'\n",
      " 'band.members' 'band.style' 'band.tours' 'dialog.holdon' 'dialog.hug'\n",
      " 'dialog.idontcare' 'dialog.sorry' 'greetings.bye' 'greetings.hello'\n",
      " 'greetings.howareyou' 'greetings.nicetomeetyou' 'greetings.nicetoseeyou'\n",
      " 'greetings.nicetotalktoyou' 'intent.outofscope' 'member.biography'\n",
      " 'song.info' 'song.lyrics' 'song.specific' 'user.angry' 'user.back'\n",
      " 'user.bored' 'user.busy' 'user.cannotsleep' 'user.excited'\n",
      " 'user.likeagent' 'user.lovesagent' 'user.needsadvice']\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-28 21:34:32,665 - training - INFO - Cross-validation completed\n",
      "2025-07-28 21:34:32,667 - training - INFO - accuracy: 0.6328 ± 0.0147\n",
      "2025-07-28 21:34:32,668 - training - INFO - macro_f1: nan ± nan\n",
      "2025-07-28 21:34:32,669 - training - INFO - Testing model on 8 test cases...\n",
      "2025-07-28 21:34:32,679 - training - INFO - 'are you a bot' -> 'agent.chatbot' (confidence: 0.074)\n",
      "2025-07-28 21:34:32,681 - training - INFO - 'bye for now' -> 'greetings.bye' (confidence: 0.071)\n",
      "2025-07-28 21:34:32,682 - training - INFO - 'Hello' -> 'agent.there' (confidence: 0.123)\n",
      "2025-07-28 21:34:32,684 - training - INFO - 'Who are the members of the band?' -> 'band.members' (confidence: 0.060)\n",
      "2025-07-28 21:34:32,685 - training - INFO - 'Tell me about quantum physics' -> 'intent.outofscope' (confidence: 0.042)\n",
      "2025-07-28 21:34:32,687 - training - INFO - 'when was RHCP formed' -> 'band.history' (confidence: 0.097)\n",
      "2025-07-28 21:34:32,688 - training - INFO - 'list their albums' -> 'album.info' (confidence: 0.114)\n",
      "2025-07-28 21:34:32,689 - training - INFO - 'name some of their songs' -> 'song.info' (confidence: 0.092)\n",
      "2025-07-28 21:34:32,691 - training - INFO - Step 6: Saving model and results...\n",
      "2025-07-28 21:34:32,708 - training - INFO - Model saved to: data/models/logistic_regression_classifier.joblib\n",
      "2025-07-28 21:34:32,709 - training - INFO - Metadata saved to: data/models/logistic_regression_classifier.json\n",
      "2025-07-28 21:34:32,721 - training - INFO - Training artifacts saved:\n",
      "2025-07-28 21:34:32,722 - training - INFO -   Model: data/models/logistic_regression_classifier.joblib\n",
      "2025-07-28 21:34:32,723 - training - INFO -   Results: data/results/training_results.json\n",
      "2025-07-28 21:34:32,724 - data_processing - INFO - Creating data backup...\n",
      "2025-07-28 21:34:32,727 - data_processing - INFO - Backup created at: data/backups/backup_20250728_213432\n",
      "2025-07-28 21:34:32,728 - training - INFO -   Backup: data/backups/backup_20250728_213432\n",
      "2025-07-28 21:34:32,729 - training - INFO - Step 7: Generating evaluation report...\n",
      "2025-07-28 21:34:32,730 - evaluation - INFO - Generating comprehensive evaluation report...\n",
      "2025-07-28 21:34:32,731 - evaluation - INFO - Generating confusion matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:971: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 152, in __call__\n",
      "    score = scorer._score(\n",
      "            ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 400, in _score\n",
      "    y_pred = method_caller(\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_scorer.py\", line 90, in _cached_call\n",
      "    result, _ = _get_response_values(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/utils/_response.py\", line 207, in _get_response_values\n",
      "    raise ValueError(\n",
      "ValueError: pos_label=1 is not a valid label: It should be one of ['agent.acquaintance' 'agent.age' 'agent.annoying' 'agent.bad'\n",
      " 'agent.beautiful' 'agent.beclever' 'agent.birthday' 'agent.boring'\n",
      " 'agent.boss' 'agent.busy' 'agent.canyouhelp' 'agent.chatbot'\n",
      " 'agent.clever' 'agent.crazy' 'agent.fire' 'agent.funny' 'agent.good'\n",
      " 'agent.happy' 'agent.hobby' 'agent.hungry' 'agent.marryuser'\n",
      " 'agent.myfriend' 'agent.occupation' 'agent.origin' 'agent.ready'\n",
      " 'agent.real' 'agent.residence' 'agent.right' 'agent.sure'\n",
      " 'agent.talktome' 'agent.there' 'album.info' 'album.specific'\n",
      " 'appraisal.bad' 'appraisal.good' 'appraisal.noproblem'\n",
      " 'appraisal.thankyou' 'appraisal.welcome' 'appraisal.welldone'\n",
      " 'band.awards' 'band.collaborations' 'band.history' 'band.influence'\n",
      " 'band.members' 'band.style' 'band.tours' 'dialog.holdon' 'dialog.hug'\n",
      " 'dialog.idontcare' 'dialog.sorry' 'greetings.bye' 'greetings.hello'\n",
      " 'greetings.howareyou' 'greetings.nicetomeetyou' 'greetings.nicetoseeyou'\n",
      " 'greetings.nicetotalktoyou' 'intent.outofscope' 'member.biography'\n",
      " 'song.info' 'song.lyrics' 'song.specific' 'user.angry' 'user.back'\n",
      " 'user.bored' 'user.busy' 'user.cannotsleep' 'user.excited'\n",
      " 'user.likeagent' 'user.lovesagent' 'user.needsadvice']\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Model Training Script for RHCP Chatbot ML Pipeline.\n",
    "\n",
    "Main training script that coordinates data loading, model training, and evaluation.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directories to path for imports - notebook compatible\n",
    "try:\n",
    "    # For script execution\n",
    "    script_dir = Path(__file__).parent\n",
    "    project_root = script_dir.parent.parent\n",
    "except NameError:\n",
    "    # For notebook execution - assume we're in notebooks/ directory or project root\n",
    "    current_dir = Path.cwd()\n",
    "    if current_dir.name == \"notebooks\":\n",
    "        # Running from notebooks directory\n",
    "        project_root = current_dir.parent\n",
    "    elif (current_dir / \"scripts\").exists():\n",
    "        # Running from project root\n",
    "        project_root = current_dir\n",
    "    else:\n",
    "        # Try to find project root by looking for scripts directory\n",
    "        project_root = current_dir\n",
    "        while project_root != project_root.parent:\n",
    "            if (project_root / \"scripts\").exists():\n",
    "                break\n",
    "            project_root = project_root.parent\n",
    "        else:\n",
    "            raise ValueError(\"Could not find project root with 'scripts' directory\")\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "# Change working directory to project root for config file loading\n",
    "os.chdir(project_root)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Python path updated to include: {project_root}\")\n",
    "print(f\"Working directory changed to: {os.getcwd()}\")\n",
    "print(f\"Config directory exists: {(Path('config') / 'training_config.yaml').exists()}\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scripts.data.enhance_data import DataEnhancer\n",
    "from scripts.data.load_data import DataLoader\n",
    "from scripts.evaluation.evaluate_model import ModelEvaluator\n",
    "from scripts.utils.config_manager import ConfigManager\n",
    "from scripts.utils.logger_setup import setup_training_logger\n",
    "from scripts.utils.model_utils import ModelUtils\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Main class for coordinating the training pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the trainer.\"\"\"\n",
    "        self.config_manager = ConfigManager()\n",
    "        self.training_config = self.config_manager.get_training_config()\n",
    "        self.data_config = self.config_manager.get_data_config()\n",
    "\n",
    "        # Set up logging\n",
    "        self.logger = setup_training_logger(self.training_config)\n",
    "\n",
    "        # Initialize components\n",
    "        self.data_loader = DataLoader(self.config_manager)\n",
    "        self.data_enhancer = DataEnhancer(self.config_manager)\n",
    "        self.evaluator = ModelEvaluator(self.config_manager)\n",
    "\n",
    "        self.logger.info(\"Model trainer initialized\")\n",
    "\n",
    "    def run_training_pipeline(self):\n",
    "        \"\"\"Run the complete training pipeline.\"\"\"\n",
    "        self.logger.info(\"=== STARTING TRAINING PIPELINE ===\")\n",
    "\n",
    "        try:\n",
    "            # Step 1: Load and validate data\n",
    "            self.logger.info(\"Step 1: Loading training data...\")\n",
    "            df = self.data_loader.load_training_data()\n",
    "\n",
    "            # Step 2: Enhance data if configured\n",
    "            enhancement_config = self.training_config.get(\"enhancement\", {})\n",
    "            if enhancement_config.get(\"enable_minority_class_enhancement\", False):\n",
    "                self.logger.info(\"Step 2: Enhancing minority classes...\")\n",
    "                df = self.data_enhancer.enhance_minority_classes(df)\n",
    "            else:\n",
    "                self.logger.info(\n",
    "                    \"Step 2: Skipping data enhancement (disabled in config)\"\n",
    "                )\n",
    "\n",
    "            # Step 3: Split data\n",
    "            self.logger.info(\"Step 3: Splitting data into train/test sets...\")\n",
    "            X_train, X_test, y_train, y_test = self._split_data(df)\n",
    "\n",
    "            # Step 4: Create and train model\n",
    "            self.logger.info(\"Step 4: Creating and training model...\")\n",
    "            pipeline = ModelUtils.create_pipeline(self.training_config)\n",
    "\n",
    "            # Set random seed for reproducibility\n",
    "            self._set_random_seed()\n",
    "\n",
    "            # Train the model\n",
    "            trained_pipeline = ModelUtils.train_model(\n",
    "                pipeline, X_train, y_train, self.training_config, self.logger\n",
    "            )\n",
    "\n",
    "            # Step 5: Evaluate model\n",
    "            self.logger.info(\"Step 5: Evaluating model performance...\")\n",
    "\n",
    "            # Test set evaluation\n",
    "            test_results = ModelUtils.evaluate_model(\n",
    "                trained_pipeline, X_test, y_test, self.training_config, self.logger\n",
    "            )\n",
    "\n",
    "            # Cross-validation\n",
    "            cv_results = ModelUtils.cross_validate_model(\n",
    "                trained_pipeline,\n",
    "                df[\"text\"],\n",
    "                df[\"intent\"],\n",
    "                self.training_config,\n",
    "                self.logger,\n",
    "            )\n",
    "\n",
    "            # Test on specific cases\n",
    "            test_cases = self.training_config.get(\"evaluation\", {}).get(\n",
    "                \"test_cases\", []\n",
    "            )\n",
    "            prediction_results = ModelUtils.test_model_predictions(\n",
    "                trained_pipeline, test_cases, self.logger\n",
    "            )\n",
    "\n",
    "            # Step 6: Save model and results\n",
    "            self.logger.info(\"Step 6: Saving model and results...\")\n",
    "            self._save_training_artifacts(\n",
    "                trained_pipeline, test_results, cv_results, prediction_results, df\n",
    "            )\n",
    "\n",
    "            # Step 7: Generate comprehensive evaluation\n",
    "            self.logger.info(\"Step 7: Generating evaluation report...\")\n",
    "            self.evaluator.generate_evaluation_report(\n",
    "                trained_pipeline, X_test, y_test, test_results, cv_results\n",
    "            )\n",
    "\n",
    "            self.logger.info(\"=== TRAINING PIPELINE COMPLETED SUCCESSFULLY ===\")\n",
    "            return trained_pipeline, test_results, cv_results\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Training pipeline failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _split_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Split data into training and test sets.\"\"\"\n",
    "        training_config = self.training_config[\"training\"]\n",
    "\n",
    "        X = df[\"text\"]\n",
    "        y = df[\"intent\"]\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=training_config[\"test_size\"],\n",
    "            random_state=training_config[\"random_state\"],\n",
    "            stratify=y if training_config[\"stratify\"] else None,\n",
    "        )\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Data split: {len(X_train)} train, {len(X_test)} test samples\"\n",
    "        )\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _set_random_seed(self):\n",
    "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "        reproducibility_config = self.training_config.get(\"reproducibility\", {})\n",
    "\n",
    "        if reproducibility_config.get(\"set_global_seed\", True):\n",
    "            seed = reproducibility_config.get(\"seed\", 42)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "            # Set other random seeds if needed\n",
    "            import random\n",
    "\n",
    "            random.seed(seed)\n",
    "\n",
    "            self.logger.info(f\"Random seed set to: {seed}\")\n",
    "\n",
    "    def _save_training_artifacts(\n",
    "        self, pipeline, test_results, cv_results, prediction_results, df\n",
    "    ):\n",
    "        \"\"\"Save all training artifacts.\"\"\"\n",
    "        output_config = self.training_config[\"output\"]\n",
    "        models_path = self.data_config[\"paths\"][\"models\"]\n",
    "        results_path = self.data_config[\"paths\"][\"results\"]\n",
    "\n",
    "        # Create paths\n",
    "        Path(models_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(results_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Model file path\n",
    "        model_path = Path(models_path) / output_config[\"model_filename\"]\n",
    "\n",
    "        # Create comprehensive metadata\n",
    "        training_info = {\n",
    "            \"total_samples\": len(df),\n",
    "            \"training_samples\": test_results[\"n_test_samples\"] * 4,  # Approximate\n",
    "            \"test_samples\": test_results[\"n_test_samples\"],\n",
    "            \"version\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            \"enhancement_applied\": self.training_config.get(\"enhancement\", {}).get(\n",
    "                \"enable_minority_class_enhancement\", False\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        metadata = ModelUtils.create_model_metadata(\n",
    "            pipeline, test_results, self.training_config, training_info\n",
    "        )\n",
    "\n",
    "        # Add cross-validation results to metadata\n",
    "        metadata[\"cross_validation\"] = cv_results\n",
    "        metadata[\"test_predictions\"] = prediction_results\n",
    "\n",
    "        # Save model with metadata\n",
    "        ModelUtils.save_model(\n",
    "            pipeline, str(model_path), metadata, self.training_config, self.logger\n",
    "        )\n",
    "\n",
    "        # Save detailed results\n",
    "        results_data = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"model_path\": str(model_path),\n",
    "            \"test_evaluation\": test_results,\n",
    "            \"cross_validation\": cv_results,\n",
    "            \"test_predictions\": prediction_results,\n",
    "            \"data_summary\": {\n",
    "                \"total_samples\": len(df),\n",
    "                \"unique_intents\": df[\"intent\"].nunique(),\n",
    "                \"class_distribution\": df[\"intent\"].value_counts().to_dict(),\n",
    "            },\n",
    "            \"configuration\": self.training_config,\n",
    "        }\n",
    "\n",
    "        results_file = Path(results_path) / output_config[\"results_filename\"]\n",
    "        with open(results_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        self.logger.info(\"Training artifacts saved:\")\n",
    "        self.logger.info(f\"  Model: {model_path}\")\n",
    "        self.logger.info(f\"  Results: {results_file}\")\n",
    "\n",
    "        # Create backup if configured\n",
    "        if output_config.get(\"create_backup\", True):\n",
    "            training_files = self.training_config[\"data\"][\"training_files\"]\n",
    "            backup_dir = self.data_loader.create_data_backup()\n",
    "            self.logger.info(f\"  Backup: {backup_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for standalone execution.\"\"\"\n",
    "    print(\"RHCP Chatbot Model Training Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Initialize and run trainer\n",
    "        trainer = ModelTrainer()\n",
    "        pipeline, test_results, cv_results = trainer.run_training_pipeline()\n",
    "\n",
    "        # Print summary\n",
    "        print(\"\\nTRAINING COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"\\nPERFORMANCE SUMMARY:\")\n",
    "        print(f\"  Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "        print(f\"  Test Macro F1: {test_results['macro_f1']:.4f}\")\n",
    "        print(\n",
    "            f\"  CV Accuracy: {cv_results['accuracy']['mean']:.4f} ± {cv_results['accuracy']['std']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  CV Macro F1: {cv_results['macro_f1']['mean']:.4f} ± {cv_results['macro_f1']['std']:.4f}\"\n",
    "        )\n",
    "\n",
    "        print(\"\\nModel is ready for deployment!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTRAINING FAILED: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c436e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to get the fixed tokenizer\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Reload the model utilities module\n",
    "if \"scripts.utils.model_utils\" in sys.modules:\n",
    "    importlib.reload(sys.modules[\"scripts.utils.model_utils\"])\n",
    "\n",
    "# Re-import the classes\n",
    "\n",
    "print(\"Modules reloaded with fixed tokenizer!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of the fixed model training and saving\n",
    "print(\"Testing fixed tokenizer and model saving...\")\n",
    "\n",
    "# Force reload of all relevant modules\n",
    "import importlib\n",
    "\n",
    "modules_to_reload = [\n",
    "    \"scripts.utils.model_utils\",\n",
    "    \"scripts.utils.config_manager\",\n",
    "    \"scripts.data.load_data\",\n",
    "    \"scripts.data.enhance_data\",\n",
    "    \"scripts.evaluation.evaluate_model\",\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        print(f\"Reloading {module_name}...\")\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "# Re-import everything fresh\n",
    "\n",
    "print(\"All modules reloaded!\")\n",
    "\n",
    "try:\n",
    "    # Create a completely fresh trainer instance\n",
    "    print(\"Creating fresh trainer instance...\")\n",
    "    trainer = ModelTrainer()\n",
    "\n",
    "    # Run the full pipeline again\n",
    "    print(\"Running training pipeline...\")\n",
    "    pipeline, test_results, cv_results = trainer.run_training_pipeline()\n",
    "\n",
    "    print(\"\\nSUCCESS! Training completed without errors!\")\n",
    "    print(f\"Test Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"Test Macro F1: {test_results['macro_f1']:.4f}\")\n",
    "    print(\n",
    "        f\"CV Accuracy: {cv_results['accuracy']['mean']:.4f} ± {cv_results['accuracy']['std']:.4f}\"\n",
    "    )\n",
    "    print(\"Model saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\n",
    "        \"\\nIf still getting pickle error, please restart the kernel and run all cells fresh!\"\n",
    "    )\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ALTERNATIVE APPROACH: Testing Fixed Tokenizer Directly ===\n",
      "2025-07-28 21:13:36,233 - data_processing - INFO - Starting data loading process...\n",
      "2025-07-28 21:13:36,234 - data_processing - INFO - Loading data from 2 files: ['data/processed/base-corpus.json', 'data/processed/rhcp-corpus.json']\n",
      "2025-07-28 21:13:36,239 - data_processing - INFO - Loaded 1041 samples with 70 unique intents\n",
      "2025-07-28 21:13:36,243 - data_processing - INFO - Class balance analysis:\n",
      "2025-07-28 21:13:36,245 - data_processing - INFO -   Most common class: agent.chatbot (72 samples)\n",
      "2025-07-28 21:13:36,247 - data_processing - INFO -   Least common class: greetings.nicetomeetyou (5 samples)\n",
      "2025-07-28 21:13:36,249 - data_processing - INFO -   Imbalance ratio: 14.40:1\n",
      "✅ Loaded 1041 samples\n",
      "✅ Data split: 832 train, 209 test\n",
      "✅ Pipeline created with fixed tokenizer\n",
      "🔄 Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model trained - Accuracy: 0.5885\n",
      "🔄 Testing model saving...\n",
      "✅ Model saved successfully to: data/models/test_fixed_model.joblib\n",
      "✅ Model loaded and tested - Accuracy: 0.5885\n",
      "✅ Test predictions:\n",
      "   'are you a bot' -> 'agent.chatbot'\n",
      "   'Hello' -> 'agent.there'\n",
      "   'Who are the members?' -> 'band.members'\n",
      "\n",
      "🎉 SUCCESS! Fixed tokenizer works perfectly!\n",
      "✅ Model can be trained, saved, loaded, and used for predictions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Alternative: Direct test of fixed tokenizer and model saving\n",
    "print(\"=== ALTERNATIVE APPROACH: Testing Fixed Tokenizer Directly ===\")\n",
    "\n",
    "try:\n",
    "    from pathlib import Path\n",
    "\n",
    "    import joblib\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    from scripts.data.load_data import DataLoader\n",
    "    from scripts.utils.config_manager import ConfigManager\n",
    "    from scripts.utils.model_utils import ModelUtils\n",
    "\n",
    "    # Load configuration and data\n",
    "    config_manager = ConfigManager()\n",
    "    training_config = config_manager.get_training_config()\n",
    "    data_config = config_manager.get_data_config()\n",
    "\n",
    "    # Load data\n",
    "    data_loader = DataLoader(config_manager)\n",
    "    df = data_loader.load_training_data()\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df[\"text\"], df[\"intent\"], test_size=0.2, random_state=42, stratify=df[\"intent\"]\n",
    "    )\n",
    "    print(f\"Data split: {len(X_train)} train, {len(X_test)} test\")\n",
    "\n",
    "    # Create pipeline with fixed tokenizer\n",
    "    pipeline = ModelUtils.create_pipeline(training_config)\n",
    "    print(\"Pipeline created with fixed tokenizer\")\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Test the model\n",
    "    accuracy = pipeline.score(X_test, y_test)\n",
    "    print(f\"Model trained - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Test saving (this is where the pickle error occurred before)\n",
    "    print(\"Testing model saving...\")\n",
    "    models_path = Path(\"data/models\")\n",
    "    models_path.mkdir(parents=True, exist_ok=True)\n",
    "    model_file = models_path / \"test_fixed_model.joblib\"\n",
    "\n",
    "    joblib.dump(pipeline, model_file)\n",
    "    print(f\"Model saved successfully to: {model_file}\")\n",
    "\n",
    "    # Test loading\n",
    "    loaded_pipeline = joblib.load(model_file)\n",
    "    test_accuracy = loaded_pipeline.score(X_test, y_test)\n",
    "    print(f\"Model loaded and tested - Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    # Test predictions\n",
    "    test_cases = [\"are you a bot\", \"Hello\", \"Who are the members?\"]\n",
    "    predictions = loaded_pipeline.predict(test_cases)\n",
    "    print(\"Test predictions:\")\n",
    "    for text, pred in zip(test_cases, predictions, strict=False):\n",
    "        print(f\"   '{text}' -> '{pred}'\")\n",
    "\n",
    "    print(\"\\nSUCCESS! Fixed tokenizer works perfectly!\")\n",
    "    print(\"Model can be trained, saved, loaded, and used for predictions!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in direct test: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b2913a2f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🔧 Pickle Error Fix Summary\n",
    "\n",
    "If you're still getting the pickle error after running the cells above, here are your options:\n",
    "\n",
    "### ✅ **Option 1: Restart Kernel (Recommended)**\n",
    "1. Click **Kernel** → **Restart** in Jupyter\n",
    "2. Run all cells from the beginning\n",
    "3. The fixed tokenizer will be used from the start\n",
    "\n",
    "### ✅ **Option 2: Use Cell 3 (Alternative Approach)**\n",
    "- Cell 3 bypasses the ModelTrainer class entirely  \n",
    "- Tests the fixed tokenizer directly\n",
    "- Should work even with module reload issues\n",
    "\n",
    "### 🐛 **Why This Happened**\n",
    "- The `ModelTrainer` class was imported in Cell 0 before the fix\n",
    "- Even after reloading modules, the old class definition persists\n",
    "- The nested function tokenizer couldn't be pickled by joblib\n",
    "\n",
    "### ✅ **What Was Fixed**  \n",
    "- Replaced nested function with `TextTokenizer` class\n",
    "- Made tokenizer properly serializable\n",
    "- Model can now be saved and loaded without errors\n",
    "\n",
    "**Bottom line**: Your model training works perfectly - just need a fresh start to use the fixed version!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbde58c-a49c-4d1a-bf1f-25cf4467557e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
