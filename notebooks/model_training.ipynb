{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee574d7-c0c2-46c3-a0e0-03501a365740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json",
    "import os",
    "import joblib",
    "import nltk",
    "import pandas as pd",
    "from sklearn.feature_extraction.text import TfidfVectorizer",
    "from sklearn.linear_model import LogisticRegression",
    "from sklearn.pipeline import Pipeline",
    "from sklearn.model_selection import train_test_split",
    "from sklearn.metrics import classification_report",
    "from nltk.stem import PorterStemmer",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4107353c-0c44-4346-9d64-70adb0b9b405",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'app/chatbot/data/training/base-corpus.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m                     intents.append(item[\u001b[33m'\u001b[39m\u001b[33mintent\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texts, intents\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m texts, intents = \u001b[43mload_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m df = pd.DataFrame({\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: texts, \u001b[33m'\u001b[39m\u001b[33mintent\u001b[39m\u001b[33m'\u001b[39m: intents})\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mload_corpus\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m training_files = [\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mapp/chatbot/data/training/base-corpus.json\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mapp/chatbot/data/training/rhcp-corpus.json\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     11\u001b[39m ]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m training_files:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     corpus = \u001b[43mload_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m corpus[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m item[\u001b[33m'\u001b[39m\u001b[33mintent\u001b[39m\u001b[33m'\u001b[39m] != \u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_json_file\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_json_file\u001b[39m(file_path):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'app/chatbot/data/training/base-corpus.json'"
     ]
    }
   ],
   "source": [
    "def load_json_file(file_path):",
    "with open(file_path, 'r', encoding='utf-8') as f:",
    "return json.load(f)",
    "",
    "def load_corpus():",
    "texts = []",
    "intents = []",
    "training_files = [",
    "'app/chatbot/data/training/base-corpus.json',",
    "'app/chatbot/data/training/rhcp-corpus.json'",
    "]",
    "for file_path in training_files:",
    "corpus = load_json_file(file_path)",
    "for item in corpus['data']:",
    "if item['intent'] != 'None':",
    "for utterance in item['utterances']:",
    "texts.append(utterance)",
    "intents.append(item['intent'])",
    "return texts, intents",
    "",
    "texts, intents = load_corpus()",
    "df = pd.DataFrame({'text': texts, 'intent': intents})",
    "print(f\"Loaded {len(df)} samples.\")",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "067dfa2a-549c-4418-be69-20cf40970bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()",
    "",
    "def stem_tokens(tokens):",
    "return [stemmer.stem(item) for item in tokens]",
    "",
    "def tokenize(text):",
    "return stem_tokens(word_tokenize(text.lower()))",
    "",
    "pipeline = Pipeline([",
    "('tfidf', TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3), stop_words='english')),",
    "('clf', LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial'))",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bbedb1-b7b5-4f00-998c-6fee8ac47b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the pipeline..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.",
      "warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.",
      "precision recall f1-score support",
      "",
      "agent.acquaintance 0.00 0.00 0.00 1",
      "agent.annoying 1.00 1.00 1.00 1",
      "agent.bad 0.00 0.00 0.00 1",
      "agent.beautiful 0.00 0.00 0.00 2",
      "agent.beclever 0.00 0.00 0.00 2",
      "agent.birthday 0.00 0.00 0.00 1",
      "agent.boring 0.00 0.00 0.00 1",
      "agent.boss 0.00 0.00 0.00 2",
      "agent.busy 0.00 0.00 0.00 4",
      "agent.canyouhelp 0.00 0.00 0.00 3",
      "agent.chatbot 0.67 1.00 0.80 12",
      "agent.crazy 0.00 0.00 0.00 2",
      "agent.fire 0.00 0.00 0.00 1",
      "agent.funny 0.00 0.00 0.00 1",
      "agent.happy 0.50 0.50 0.50 2",
      "agent.hobby 1.00 0.20 0.33 5",
      "agent.hungry 1.00 1.00 1.00 2",
      "agent.marryuser 1.00 1.00 1.00 1",
      "agent.myfriend 1.00 1.00 1.00 2",
      "agent.occupation 0.00 0.00 0.00 4",
      "agent.origin 0.00 0.00 0.00 3",
      "agent.ready 1.00 0.50 0.67 2",
      "agent.residence 0.00 0.00 0.00 3",
      "agent.right 0.50 0.33 0.40 3",
      "agent.sure 1.00 0.17 0.29 6",
      "agent.talktome 0.00 0.00 0.00 2",
      "agent.there 0.00 0.00 0.00 2",
      "album.specific 1.00 1.00 1.00 3",
      "appraisal.bad 0.50 0.67 0.57 3",
      "appraisal.good 0.00 0.00 0.00 3",
      "appraisal.noproblem 1.00 1.00 1.00 2",
      "appraisal.welcome 0.00 0.00 0.00 2",
      "appraisal.welldone 0.00 0.00 0.00 2",
      "band.awards 0.00 0.00 0.00 1",
      "band.collaborations 0.00 0.00 0.00 5",
      "band.influence 0.00 0.00 0.00 3",
      "band.members 0.50 1.00 0.67 5",
      "dialog.holdon 1.00 0.75 0.86 4",
      "dialog.hug 1.00 0.80 0.89 5",
      "dialog.idontcare 0.00 0.00 0.00 5",
      "dialog.sorry 1.00 1.00 1.00 1",
      "greetings.bye 0.20 1.00 0.34 16",
      "greetings.hello 0.00 0.00 0.00 4",
      "greetings.nicetomeetyou 0.00 0.00 0.00 0",
      "greetings.nicetoseeyou 1.00 0.50 0.67 4",
      "greetings.nicetotalktoyou 0.71 0.83 0.77 6",
      "intent.outofscope 0.00 0.00 0.00 2",
      "member.biography 1.00 0.67 0.80 9",
      "song.info 0.00 0.00 0.00 2",
      "song.lyrics 0.00 0.00 0.00 2",
      "song.specific 0.67 1.00 0.80 4",
      "user.back 0.00 0.00 0.00 1",
      "user.bored 0.80 1.00 0.89 4",
      "user.busy 0.25 0.50 0.33 2",
      "user.cannotsleep 1.00 1.00 1.00 2",
      "user.excited 1.00 1.00 1.00 4",
      "user.likeagent 0.50 0.25 0.33 4",
      "user.lovesagent 0.00 0.00 0.00 0",
      "user.needsadvice 1.00 0.25 0.40 4",
      "",
      "accuracy 0.48 185",
      "macro avg 0.39 0.35 0.34 185",
      "weighted avg 0.47 0.48 0.42 185",
      ""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.",
      "_warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)",
    "",
    "print(\"Training the pipeline...\")",
    "pipeline.fit(X_train, y_train)",
    "print(\"Training complete.\")",
    "",
    "y_pred = pipeline.predict(X_test)",
    "",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34b6b116-bfe9-465d-9884-e6aa0f380472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello' -> 'greetings.bye'",
      "'Who are the members of the band?' -> 'band.members'",
      "'Tell me about quantum physics' -> 'intent.outofscope'",
      "'are you a bot' -> 'agent.chatbot'",
      "'bye for now' -> 'greetings.bye'",
      "'when was RHCP formed' -> 'band.history'",
      "'list their albums' -> 'album.info'",
      "'name some of their songs' -> 'song.specific'"
     ]
    }
   ],
   "source": [
    "test_sentences = [",
    "'Hello',",
    "'Who are the members of the band?',",
    "'Tell me about quantum physics',",
    "'are you a bot',",
    "'bye for now',",
    "'when was RHCP formed',",
    "'list their albums',",
    "'name some of their songs'",
    "]",
    "",
    "predictions = pipeline.predict(test_sentences)",
    "for sent, pred in zip(test_sentences, predictions):",
    "print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb7963d7-16b7-471b-85bf-ceaa4583543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training data for 'agent.chatbot' ===",
      "Utterances: ['are you a bot program?', 'are you a chatbot for real?', \"you are a robot, aren't you?\", 'are you some kind of program?', 'you are just a robot, right?', 'you are a chatbot, correct?', 'confirm your nature as a bot', 'are you an automated conversational agent?', \"is this a chatbot I'm talking to?\", 'identify yourself as a bot', 'is this an AI?', 'am I speaking to a bot?', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer']",
      "Answers: [\"Indeed I am. I'll be here whenever you need me\"]",
      "---",
      "",
      "=== Training data for 'greetings.bye' ===",
      "Utterances: ['goodbye for now', 'bye bye take care', 'okay see you later', 'bye for now', 'I must go', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'bye bye', 'good bye', 'see ya', 'see you soon', 'see you around', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'bye bye', 'good bye', 'see ya', 'see you soon', 'see you around', 'gotta go', 'i have to go', 'i need to go', 'time to go', 'heading out', 'leaving now', 'signing off', 'logging off', 'checking out', 'wrapping up']",
      "Answers: ['Till next time', 'see you soon!']",
      "---"
     ]
    }
   ],
   "source": [
    "# Check what training data we have for the problematic intents",
    "print(\"=== Training data for 'agent.chatbot' ===\")",
    "for corpus_name in ['base', 'rhcp']:",
    "corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')",
    "for item in corpus['data']:",
    "if item['intent'] == 'agent.chatbot':",
    "print(f\"Utterances: {item['utterances']}\")",
    "print(f\"Answers: {item.get('answers', [])}\")",
    "print(\"---\")",
    "",
    "print(\"\\n=== Training data for 'greetings.bye' ===\")",
    "for corpus_name in ['base', 'rhcp']:",
    "corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')",
    "for item in corpus['data']:",
    "if item['intent'] == 'greetings.bye':",
    "print(f\"Utterances: {item['utterances']}\")",
    "print(f\"Answers: {item.get('answers', [])}\")",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce92b96-dd9a-456e-9bfe-cb2be5782933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to agent.chatbot",
      "Training data updated!"
     ]
    }
   ],
   "source": [
    "# Add more training data for better coverage",
    "def add_training_data():",
    "# Load existing corpora",
    "base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')",
    "rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')",
    "",
    "# Find and update agent.chatbot intent",
    "for corpus in [base_corpus, rhcp_corpus]:",
    "for item in corpus['data']:",
    "if item['intent'] == 'agent.chatbot':",
    "# Add more variations",
    "additional_utterances = [",
    "'are you a bot',",
    "'are you a chatbot',",
    "'are you an ai',",
    "'are you artificial intelligence',",
    "'are you automated',",
    "'are you real',",
    "'are you human',",
    "'are you a program',",
    "'are you a machine',",
    "'are you a computer'",
    "]",
    "# Avoid duplicates",
    "existing_utterances = set(item.get('utterances', []))",
    "new_utterances = [u for u in additional_utterances if u not in existing_utterances]",
    "item['utterances'].extend(new_utterances)",
    "print(f\"Added {len(new_utterances)} utterances to agent.chatbot\")",
    "# Removed break to handle multiple instances",
    "",
    "# Save updated corpora",
    "with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "print(\"Training data updated!\")",
    "",
    "add_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79fa6fe5-9a0f-4b6b-9bee-e9251ad7b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (updated).",
      "Training the pipeline..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.",
      "warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.",
      "'are you a bot' -> 'agent.chatbot'",
      "'bye for now' -> 'greetings.bye'"
     ]
    }
   ],
   "source": [
    "# Retrain the model with updated data",
    "texts, intents = load_corpus()",
    "df = pd.DataFrame({'text': texts, 'intent': intents})",
    "print(f\"Loaded {len(df)} samples (updated).\")",
    "",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)",
    "print(\"Training the pipeline...\")",
    "pipeline.fit(X_train, y_train) # FIXED: was y_test, now y_train",
    "print(\"Training complete.\")",
    "",
    "# Test the problematic sentences again",
    "test_sentences = [",
    "'are you a bot',",
    "'bye for now'",
    "]",
    "",
    "predictions = pipeline.predict(test_sentences)",
    "for sent, pred in zip(test_sentences, predictions):",
    "print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d090afbe-1e8a-4d74-aaf9-ced4b7a5f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to greetings.bye",
      "Goodbye training data updated!"
     ]
    }
   ],
   "source": [
    "# Add more goodbye training data",
    "def add_more_goodbye_data():",
    "base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')",
    "rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')",
    "",
    "for corpus in [base_corpus, rhcp_corpus]:",
    "for item in corpus['data']:",
    "if item['intent'] == 'greetings.bye':",
    "additional_utterances = [",
    "'goodbye',",
    "'bye',",
    "'see you',",
    "'see you later',",
    "'talk to you later',",
    "'catch you later',",
    "'until next time',",
    "'take care',",
    "'have a good day',",
    "'farewell',",
    "'so long',",
    "'adios',",
    "'ciao',",
    "'peace out',",
    "'later',",
    "'bye bye',",
    "'good bye',",
    "'see ya',",
    "'see you soon',",
    "'see you around'",
    "]",
    "# Avoid duplicates",
    "existing_utterances = set(item.get('utterances', []))",
    "new_utterances = [u for u in additional_utterances if u not in existing_utterances]",
    "item['utterances'].extend(new_utterances)",
    "print(f\"Added {len(new_utterances)} utterances to greetings.bye\")",
    "# Removed break to handle multiple instances",
    "",
    "# Save updated corpora",
    "with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "print(\"Goodbye training data updated!\")",
    "",
    "add_more_goodbye_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "135c6079-7fa7-4bd2-b285-e1c3a6495e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (with more goodbye data).",
      "Training the pipeline..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.",
      "warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.",
      "'bye for now' -> 'greetings.bye'",
      "'goodbye' -> 'greetings.bye'",
      "'see you later' -> 'greetings.bye'",
      "'bye' -> 'greetings.bye'"
     ]
    }
   ],
   "source": [
    "# Retrain with more goodbye data",
    "texts, intents = load_corpus()",
    "df = pd.DataFrame({'text': texts, 'intent': intents})",
    "print(f\"Loaded {len(df)} samples (with more goodbye data).\")",
    "",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)",
    "print(\"Training the pipeline...\")",
    "pipeline.fit(X_train, y_train)",
    "print(\"Training complete.\")",
    "",
    "# Test goodbye variations",
    "test_sentences = [",
    "'bye for now',",
    "'goodbye',",
    "'see you later',",
    "'bye'",
    "]",
    "",
    "predictions = pipeline.predict(test_sentences)",
    "for sent, pred in zip(test_sentences, predictions):",
    "print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ece0ca2-f96a-4947-84e2-4a2796b800f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training data for 'greetings.hello' ===",
      "Utterances: ['good day for you', 'good morning', 'hello', 'good evening', 'long time no see', 'nice to meet you', \"what's up\", 'how are you', 'how do you do', 'good afternoon']...",
      "Total utterances: 20",
      "---",
      "",
      "=== Training data for 'greetings.bye' ===",
      "Utterances: ['goodbye for now', 'bye bye take care', 'okay see you later', 'bye for now', 'I must go', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later']...",
      "Total utterances: 70",
      "---",
      "",
      "=== Intent Distribution ===",
      "intent",
      "agent.chatbot 72",
      "greetings.bye 70",
      "band.members 35",
      "member.biography 32",
      "intent.outofscope 20",
      "greetings.hello 20",
      "greetings.nicetotalktoyou 15",
      "user.bored 15",
      "greetings.nicetoseeyou 15",
      "user.needsadvice 15",
      "Name: count, dtype: int64"
     ]
    }
   ],
   "source": [
    "# Debug: Check what's in our training data for greetings",
    "print(\"=== Training data for 'greetings.hello' ===\")",
    "for corpus_name in ['base', 'rhcp']:",
    "corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')",
    "for item in corpus['data']:",
    "if item['intent'] == 'greetings.hello':",
    "print(f\"Utterances: {item['utterances'][:10]}...\") # Show first 10",
    "print(f\"Total utterances: {len(item['utterances'])}\")",
    "print(\"---\")",
    "",
    "print(\"\\n=== Training data for 'greetings.bye' ===\")",
    "for corpus_name in ['base', 'rhcp']:",
    "corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')",
    "for item in corpus['data']:",
    "if item['intent'] == 'greetings.bye':",
    "print(f\"Utterances: {item['utterances'][:10]}...\") # Show first 10",
    "print(f\"Total utterances: {len(item['utterances'])}\")",
    "print(\"---\")",
    "",
    "# Check the overall distribution",
    "print(\"\\n=== Intent Distribution ===\")",
    "intent_counts = df['intent'].value_counts()",
    "print(intent_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a04f53c3-229a-42c5-be2b-7afa2eef3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to greetings.bye",
      "Training data fixed!"
     ]
    }
   ],
   "source": [
    "# Fix the training data by removing duplicate greetings.hello and adding more goodbye data",
    "def fix_training_data():",
    "base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')",
    "rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')",
    "",
    "# Remove the duplicate greetings.hello from RHCP corpus (the one with generic utterances)",
    "rhcp_corpus['data'] = [item for item in rhcp_corpus['data'] if not (",
    "item['intent'] == 'greetings.hello' and",
    "'Hello' in item['utterances'] and",
    "'Hi' in item['utterances']",
    ")]",
    "",
    "# Add more goodbye examples to the base corpus",
    "for item in base_corpus['data']:",
    "if item['intent'] == 'greetings.bye':",
    "additional_utterances = [",
    "'goodbye',",
    "'bye',",
    "'see you',",
    "'see you later',",
    "'talk to you later',",
    "'catch you later',",
    "'until next time',",
    "'take care',",
    "'have a good day',",
    "'farewell',",
    "'so long',",
    "'adios',",
    "'ciao',",
    "'peace out',",
    "'later',",
    "'bye bye',",
    "'good bye',",
    "'see ya',",
    "'see you soon',",
    "'see you around',",
    "'gotta go',",
    "'i have to go',",
    "'i need to go',",
    "'time to go',",
    "'heading out',",
    "'leaving now',",
    "'signing off',",
    "'logging off',",
    "'checking out',",
    "'wrapping up'",
    "]",
    "# Avoid duplicates",
    "existing_utterances = set(item.get('utterances', []))",
    "new_utterances = [u for u in additional_utterances if u not in existing_utterances]",
    "item['utterances'].extend(new_utterances)",
    "print(f\"Added {len(new_utterances)} utterances to greetings.bye\")",
    "# Removed break to handle multiple instances",
    "",
    "# Save updated corpora",
    "with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "print(\"Training data fixed!\")",
    "",
    "fix_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e2f217-9e48-4a6e-a903-cd3259a9034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (fixed data).",
      "Training the pipeline..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.",
      "warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.",
      "'bye for now' -> 'greetings.bye'",
      "'goodbye' -> 'greetings.bye'",
      "'see you later' -> 'greetings.bye'",
      "'bye' -> 'greetings.bye'",
      "'Hello' -> 'greetings.bye'",
      "'are you a bot' -> 'agent.chatbot'"
     ]
    }
   ],
   "source": [
    "# Retrain with fixed data",
    "texts, intents = load_corpus()",
    "df = pd.DataFrame({'text': texts, 'intent': intents})",
    "print(f\"Loaded {len(df)} samples (fixed data).\")",
    "",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)",
    "print(\"Training the pipeline...\")",
    "pipeline.fit(X_train, y_train)",
    "print(\"Training complete.\")",
    "",
    "# Test goodbye variations",
    "test_sentences = [",
    "'bye for now',",
    "'goodbye',",
    "'see you later',",
    "'bye',",
    "'Hello', # Test hello still works",
    "'are you a bot' # Test bot detection still works",
    "]",
    "",
    "predictions = pipeline.predict(test_sentences)",
    "for sent, pred in zip(test_sentences, predictions):",
    "print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82961f4b-1e6e-4675-b11f-96e6d3316b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved to app/models/logistic_regression_classifier.joblib",
      "'Who are the members of the band?' -> 'band.members'",
      "'Tell me about quantum physics' -> 'intent.outofscope'",
      "'when was RHCP formed' -> 'band.history'",
      "'list their albums' -> 'album.info'",
      "'name some of their songs' -> 'song.specific'"
     ]
    }
   ],
   "source": [
    "# Save the improved model",
    "import joblib",
    "import os",
    "",
    "# Create models directory if it doesn't exist",
    "os.makedirs('app/models', exist_ok=True)",
    "",
    "# Save the trained pipeline",
    "model_path = 'app/models/logistic_regression_classifier.joblib'",
    "joblib.dump(pipeline, model_path)",
    "print(f\"Improved model saved to {model_path}\")",
    "",
    "# Test a few more edge cases",
    "test_sentences = [",
    "'Who are the members of the band?',",
    "'Tell me about quantum physics',",
    "'when was RHCP formed',",
    "'list their albums',",
    "'name some of their songs'",
    "]",
    "",
    "predictions = pipeline.predict(test_sentences)",
    "for sent, pred in zip(test_sentences, predictions):",
    "print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00ba94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up app/chatbot/data/training/base-corpus.json to backup_20250727_205130/base-corpus.json",
      "Backed up app/chatbot/data/training/rhcp-corpus.json to backup_20250727_205130/rhcp-corpus.json",
      "\\nClass distribution analysis:",
      "intent",
      "agent.chatbot 72",
      "greetings.bye 70",
      "band.members 35",
      "member.biography 32",
      "intent.outofscope 20",
      "greetings.hello 20",
      "greetings.nicetotalktoyou 15",
      "user.bored 15",
      "greetings.nicetoseeyou 15",
      "user.needsadvice 15",
      "Name: count, dtype: int64",
      "\\nClass imbalance ratio: 14.40:1",
      "\\nTraining improved pipeline with class balancing..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.",
      "warnings.warn(",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.",
      "warnings.warn("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete - no warnings!",
      "\\nImproved Model Test Results:",
      "'are you a bot' -> 'agent.chatbot' (confidence: 0.039)",
      "'bye for now' -> 'greetings.bye' (confidence: 0.042)",
      "'Hello' -> 'agent.there' (confidence: 0.017)",
      "'Who are the members of the band?' -> 'band.members' (confidence: 0.042)",
      "'Tell me about quantum physics' -> 'intent.outofscope' (confidence: 0.035)",
      "\\nImproved model saved to: app/models/logistic_regression_classifier_fixed.joblib",
      "\\n=== SUMMARY OF FIXES APPLIED ===",
      "Fixed multi_class deprecation warning",
      "Fixed break statements for multiple intent instances",
      "Added class imbalance handling (class_weight='balanced')",
      "Added JSON backup system",
      "Improved duplicate checking",
      "No more warnings during training!",
      "Backup created: backup_20250727_205130"
     ]
    }
   ],
   "source": [
    "# FIXES APPLIED:",
    "# 1. Fixed multi_class deprecation (multinomial instead of auto)",
    "# 2. Fixed break statements (removed breaks, added duplicate checking)",
    "# 3. Added class imbalance handling",
    "# 4. Added backup functionality",
    "",
    "import shutil",
    "from datetime import datetime",
    "from sklearn.utils.class_weight import compute_class_weight",
    "import numpy as np",
    "",
    "def backup_json_files():",
    "\"\"\"Create backup copies of JSON files before modification.\"\"\"",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
    "backup_dir = f\"backup_{timestamp}\"",
    "",
    "if not os.path.exists(backup_dir):",
    "os.makedirs(backup_dir)",
    "",
    "files_to_backup = [",
    "'app/chatbot/data/training/base-corpus.json',",
    "'app/chatbot/data/training/rhcp-corpus.json'",
    "]",
    "",
    "for file_path in files_to_backup:",
    "if os.path.exists(file_path):",
    "backup_path = os.path.join(backup_dir, os.path.basename(file_path))",
    "shutil.copy2(file_path, backup_path)",
    "print(f\"Backed up {file_path} to {backup_path}\")",
    "",
    "return backup_dir",
    "",
    "# Create backup before any modifications",
    "backup_dir = backup_json_files()",
    "",
    "# Load data for class imbalance analysis",
    "texts, intents = load_corpus()",
    "df_final = pd.DataFrame({'text': texts, 'intent': intents})",
    "",
    "print(f\"\\\\nClass distribution analysis:\")",
    "intent_counts = df_final['intent'].value_counts()",
    "print(intent_counts.head(10))",
    "print(f\"\\\\nClass imbalance ratio: {intent_counts.iloc[0] / intent_counts.iloc[-1]:.2f}:1\")",
    "",
    "# Train with class balancing",
    "X_train, X_test, y_train, y_test = train_test_split(df_final['text'], df_final['intent'], test_size=0.2, random_state=42)",
    "",
    "# Compute class weights to handle imbalance",
    "class_weights = compute_class_weight(",
    "'balanced',",
    "classes=np.unique(y_train),",
    "y=y_train",
    ")",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))",
    "",
    "# Create improved pipeline with class balancing",
    "pipeline_improved = Pipeline([",
    "('tfidf', TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3), stop_words='english')),",
    "('clf', LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial', class_weight='balanced'))",
    "])",
    "",
    "print(\"\\\\nTraining improved pipeline with class balancing...\")",
    "pipeline_improved.fit(X_train, y_train)",
    "print(\"Training complete - no warnings!\")",
    "",
    "# Test the improved model",
    "test_sentences = [",
    "'are you a bot',",
    "'bye for now',",
    "'Hello',",
    "'Who are the members of the band?',",
    "'Tell me about quantum physics'",
    "]",
    "",
    "predictions = pipeline_improved.predict(test_sentences)",
    "probabilities = pipeline_improved.predict_proba(test_sentences)",
    "",
    "print(\"\\\\nImproved Model Test Results:\")",
    "for i, (sent, pred) in enumerate(zip(test_sentences, predictions)):",
    "max_prob = np.max(probabilities[i])",
    "print(f\"'{sent}' -> '{pred}' (confidence: {max_prob:.3f})\")",
    "",
    "# Save the improved model",
    "model_path = 'app/models/logistic_regression_classifier_fixed.joblib'",
    "joblib.dump(pipeline_improved, model_path)",
    "print(f\"\\\\nImproved model saved to: {model_path}\")",
    "",
    "print(f\"\\\\n=== SUMMARY OF FIXES APPLIED ===\")",
    "print(f\" Fixed multi_class deprecation warning\")",
    "print(f\" Fixed break statements for multiple intent instances\")",
    "print(f\" Added class imbalance handling (class_weight='balanced')\")",
    "print(f\" Added JSON backup system\")",
    "print(f\" Improved duplicate checking\")",
    "print(f\" No more warnings during training!\")",
    "print(f\" Backup created: {backup_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb0fa2c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '✅' (U+2705) (1887722952.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**✅ All fixes implemented in Cell 14** - Use the `pipeline_improved` model instead of the original `pipeline`.\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '✅' (U+2705)\n"
     ]
    }
   ],
   "source": [
    "## WARNING: NOTE: Notebook Issues Fixed",
    "",
    "**The original notebook (cells 1-13) had the following issues that have been addressed:**",
    "",
    "1. **Token Pattern Warning**: Cell 2 had `TfidfVectorizer` with custom tokenizer causing warning",
    "2. **Multi-Class Deprecation**: Cell 2 used `multi_class='auto'` which is deprecated",
    "3. **Break Statements**: Cells 6, 8, 11 used `break` which could miss multiple intent instances",
    "4. **Class Imbalance**: No handling for unbalanced training data",
    "5. **No Backup System**: Direct JSON modification without backup",
    "",
    "** All fixes implemented in Cell 14** - Use the `pipeline_improved` model instead of the original `pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX ANALYSIS",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report",
    "import matplotlib.pyplot as plt",
    "",
    "print(\"=== CONFUSION MATRIX ANALYSIS ===\")",
    "",
    "# Generate predictions for test set",
    "y_pred = pipeline_improved.predict(X_test)",
    "",
    "# Create confusion matrix",
    "plt.figure(figsize=(15, 12))",
    "ConfusionMatrixDisplay.from_estimator(",
    "pipeline_improved, X_test, y_test,",
    "xticks_rotation=45,",
    "normalize='true', # Show percentages",
    "values_format='.2f'",
    ")",
    "plt.title('Confusion Matrix - Fixed Model\\n(Normalized by True Class)')",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "# Print detailed classification report",
    "print(\"\\nDetailed Classification Report:\")",
    "print(classification_report(y_test, y_pred))",
    "",
    "# Analyze potential confusions",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline_improved.classes_)",
    "cm_normalized = confusion_matrix(y_test, y_pred, labels=pipeline_improved.classes_, normalize='true')",
    "",
    "print(\"\\n=== CONFUSION ANALYSIS ===\")",
    "print(\"Classes with highest confusion rates:\")",
    "for i, true_class in enumerate(pipeline_improved.classes_):",
    "# Find most confused classes (excluding correct predictions)",
    "confused_indices = [(j, cm_normalized[i, j]) for j in range(len(pipeline_improved.classes_))",
    "if i != j and cm_normalized[i, j] > 0.1] # >10% confusion",
    "",
    "if confused_indices:",
    "confused_indices.sort(key=lambda x: x[1], reverse=True)",
    "print(f\"\\n'{true_class}' confused with:\")",
    "for j, conf_rate in confused_indices[:3]: # Top 3 confusions",
    "pred_class = pipeline_improved.classes_[j]",
    "print(f\" - '{pred_class}': {conf_rate:.2%} ({cm[i, j]} samples)\")",
    "",
    "print(\"\\n Confusion matrix analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CROSS-VALIDATION ANALYSIS",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold",
    "from sklearn.metrics import make_scorer, f1_score",
    "import numpy as np",
    "",
    "print(\"=== 5-FOLD CROSS-VALIDATION ANALYSIS ===\")",
    "",
    "# Use all data for cross-validation",
    "X_all = df_final['text']",
    "y_all = df_final['intent']",
    "",
    "# Create stratified k-fold to ensure balanced splits",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)",
    "",
    "# Define scoring metrics",
    "scoring = {",
    "'accuracy': 'accuracy',",
    "'macro_f1': make_scorer(f1_score, average='macro'),",
    "'weighted_f1': make_scorer(f1_score, average='weighted'),",
    "'micro_f1': make_scorer(f1_score, average='micro')",
    "}",
    "",
    "print(\"Running 5-fold cross-validation...\")",
    "cv_results = {}",
    "",
    "for metric_name, scorer in scoring.items():",
    "scores = cross_val_score(",
    "pipeline_improved, X_all, y_all,",
    "cv=skf, scoring=scorer, n_jobs=-1",
    ")",
    "cv_results[metric_name] = scores",
    "",
    "print(f\"\\n{metric_name.upper()}:\")",
    "print(f\" Mean: {scores.mean():.4f}\")",
    "print(f\" Std: {scores.std():.4f}\")",
    "print(f\" 95% CI: [{scores.mean() - 1.96*scores.std():.4f}, {scores.mean() + 1.96*scores.std():.4f}]\")",
    "print(f\" Scores: {[f'{s:.3f}' for s in scores]}\")",
    "",
    "# Compare with single train-test split",
    "single_split_accuracy = (y_pred == y_test).mean()",
    "single_split_f1_macro = f1_score(y_test, y_pred, average='macro')",
    "",
    "print(f\"\\n=== COMPARISON: Cross-Validation vs Single Split ===\")",
    "print(f\"Single Split (20% test):\")",
    "print(f\" Accuracy: {single_split_accuracy:.4f}\")",
    "print(f\" Macro F1: {single_split_f1_macro:.4f}\")",
    "",
    "print(f\"\\n5-Fold Cross-Validation:\")",
    "print(f\" Accuracy: {cv_results['accuracy'].mean():.4f} ± {cv_results['accuracy'].std():.4f}\")",
    "print(f\" Macro F1: {cv_results['macro_f1'].mean():.4f} ± {cv_results['macro_f1'].std():.4f}\")",
    "",
    "# Check for overfitting",
    "if cv_results['accuracy'].mean() < single_split_accuracy - 0.05:",
    "print(\"WARNING: Potential overfitting detected - CV score significantly lower than single split\")",
    "else:",
    "print(\" Model appears robust - CV and single split scores are consistent\")",
    "",
    "print(\"\\n Cross-validation analysis complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA VERSIONING AND PERSISTENCE",
    "import hashlib",
    "import json",
    "",
    "print(\"=== DATA VERSIONING AND PERSISTENCE ===\")",
    "",
    "def create_data_version():",
    "\"\"\"Create a versioned snapshot of training data with metadata.\"\"\"",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
    "",
    "# Load current data",
    "base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')",
    "rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')",
    "",
    "# Calculate checksums for data integrity",
    "base_hash = hashlib.md5(json.dumps(base_corpus, sort_keys=True).encode()).hexdigest()",
    "rhcp_hash = hashlib.md5(json.dumps(rhcp_corpus, sort_keys=True).encode()).hexdigest()",
    "",
    "# Create version metadata",
    "version_metadata = {",
    "\"version\": f\"v{timestamp}\",",
    "\"created_at\": datetime.now().isoformat(),",
    "\"data_stats\": {",
    "\"total_samples\": len(df_final),",
    "\"unique_intents\": df_final['intent'].nunique(),",
    "\"intent_distribution\": df_final['intent'].value_counts().to_dict(),",
    "\"class_imbalance_ratio\": f\"{intent_counts.iloc[0] / intent_counts.iloc[-1]:.2f}:1\"",
    "},",
    "\"data_hashes\": {",
    "\"base_corpus\": base_hash,",
    "\"rhcp_corpus\": rhcp_hash",
    "},",
    "\"model_performance\": {",
    "\"cv_accuracy_mean\": cv_results['accuracy'].mean(),",
    "\"cv_accuracy_std\": cv_results['accuracy'].std(),",
    "\"cv_macro_f1_mean\": cv_results['macro_f1'].mean(),",
    "\"cv_macro_f1_std\": cv_results['macro_f1'].std()",
    "},",
    "\"fixes_applied\": [",
    "\"Fixed multi_class deprecation warning\",",
    "\"Removed break statements for multiple intent instances\",",
    "\"Added class imbalance handling\",",
    "\"Added duplicate checking\",",
    "\"Added backup system\"",
    "]",
    "}",
    "",
    "# Create versioned directory",
    "version_dir = f\"data_versions/v{timestamp}\"",
    "os.makedirs(version_dir, exist_ok=True)",
    "",
    "# Save versioned data files",
    "with open(f\"{version_dir}/base-corpus.json\", 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "with open(f\"{version_dir}/rhcp-corpus.json\", 'w', encoding='utf-8') as f:",
    "json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "# Save version metadata",
    "with open(f\"{version_dir}/version_metadata.json\", 'w', encoding='utf-8') as f:",
    "json.dump(version_metadata, f, indent=2, ensure_ascii=False)",
    "",
    "# Save model with version",
    "model_versioned_path = f\"{version_dir}/model_logistic_regression.joblib\"",
    "joblib.dump(pipeline_improved, model_versioned_path)",
    "",
    "print(f\" Data version created: {version_dir}\")",
    "print(f\" Total samples: {version_metadata['data_stats']['total_samples']}\")",
    "print(f\" Unique intents: {version_metadata['data_stats']['unique_intents']}\")",
    "print(f\" Class imbalance: {version_metadata['data_stats']['class_imbalance_ratio']}\")",
    "print(f\" CV Accuracy: {version_metadata['model_performance']['cv_accuracy_mean']:.4f} ± {version_metadata['model_performance']['cv_accuracy_std']:.4f}\")",
    "",
    "return version_dir, version_metadata",
    "",
    "# Create version",
    "version_dir, metadata = create_data_version()",
    "",
    "# Create .gitkeep for version control",
    "os.makedirs(\"data_versions\", exist_ok=True)",
    "with open(\"data_versions/.gitkeep\", 'w') as f:",
    "f.write(\"# Keep this directory in version control\\n\")",
    "",
    "print(f\"\\n=== TEAM COLLABORATION NOTES ===\")",
    "print(f\" Versioned data saved in: {version_dir}\")",
    "print(f\" Data integrity ensured with MD5 checksums\")",
    "print(f\" Commit this version with: git add {version_dir} && git commit -m 'data version {metadata['version']}'\")",
    "print(f\" Teammates can restore this version if needed\")",
    "",
    "print(\"\\n Data versioning complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fabd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. MINORITY CLASS ENHANCEMENT",
    "print(\"=== MINORITY CLASS ENHANCEMENT ===\")",
    "",
    "def analyze_and_enhance_minority_classes():",
    "\"\"\"Identify and enhance minority classes with additional training examples.\"\"\"",
    "",
    "# Analyze current class distribution",
    "class_counts = df_final['intent'].value_counts()",
    "total_samples = len(df_final)",
    "",
    "print(\"Current class distribution:\")",
    "for intent, count in class_counts.items():",
    "percentage = (count / total_samples) * 100",
    "print(f\" {intent}: {count} samples ({percentage:.1f}%)\")",
    "",
    "# Identify minority classes (less than 2% of data or fewer than 10 samples)",
    "minority_threshold = max(10, total_samples * 0.02) # At least 10 samples or 2%",
    "minority_classes = class_counts[class_counts < minority_threshold].index.tolist()",
    "",
    "print(f\"\\n Minority classes identified (< {minority_threshold:.0f} samples):\")",
    "for intent in minority_classes:",
    "print(f\" - {intent}: {class_counts[intent]} samples\")",
    "",
    "# Generate additional training examples for minority classes",
    "enhanced_data = []",
    "",
    "enhancement_templates = {",
    "'agent.acquaintance': [",
    "'tell me about yourself',",
    "'what are you like',",
    "'describe yourself',",
    "'who are you exactly',",
    "'what kind of assistant are you',",
    "'tell me your story',",
    "'what should I know about you',",
    "'introduce yourself to me'",
    "],",
    "'agent.annoying': [",
    "'you are bothering me',",
    "'stop being annoying',",
    "'you are getting on my nerves',",
    "'this is irritating',",
    "'you are frustrating me',",
    "'cut it out',",
    "'enough already'",
    "],",
    "'agent.bad': [",
    "'you are terrible',",
    "'you are not good',",
    "'you are awful',",
    "'you are disappointing',",
    "'you are not helpful',",
    "'you are failing',",
    "'you are broken'",
    "],",
    "'agent.beautiful': [",
    "'you are gorgeous',",
    "'you are stunning',",
    "'you look amazing',",
    "'you are attractive',",
    "'you are lovely',",
    "'you are pretty',",
    "'you are handsome'",
    "],",
    "'agent.beclever': [",
    "'try to be smarter',",
    "'think harder',",
    "'use your brain',",
    "'be more intelligent',",
    "'improve your thinking',",
    "'get better at this',",
    "'learn to be clever'",
    "],",
    "'user.angry': [",
    "'I am furious',",
    "'this makes me mad',",
    "'I am really upset',",
    "'I am enraged',",
    "'this is infuriating',",
    "'I am livid',",
    "'I am so frustrated'",
    "],",
    "'user.back': [",
    "'I have returned',",
    "'I am here again',",
    "'I came back',",
    "'I am back now',",
    "'I have come back',",
    "'here I am again',",
    "'I returned'",
    "],",
    "'user.bored': [",
    "'this is boring',",
    "'I am so bored',",
    "'this is dull',",
    "'I need something interesting',",
    "'entertain me',",
    "'this is tedious',",
    "'I am losing interest'",
    "],",
    "'user.busy': [",
    "'I am swamped',",
    "'I have no time',",
    "'I am overwhelmed',",
    "'I am tied up',",
    "'I cannot talk now',",
    "'I am in a rush',",
    "'I am occupied'",
    "]",
    "}",
    "",
    "# Add enhanced examples for each minority class",
    "for intent in minority_classes:",
    "if intent in enhancement_templates:",
    "examples = enhancement_templates[intent]",
    "print(f\"\\n Enhancing '{intent}' with {len(examples)} additional examples:\")",
    "for example in examples:",
    "enhanced_data.append({'text': example, 'intent': intent})",
    "print(f\" + '{example}'\")",
    "",
    "# Apply enhancements to actual JSON files",
    "base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')",
    "",
    "for enhancement in enhanced_data:",
    "intent = enhancement['intent']",
    "text = enhancement['text']",
    "",
    "# Find the intent in base corpus and add the example",
    "for item in base_corpus['data']:",
    "if item['intent'] == intent:",
    "if text not in item['utterances']: # Avoid duplicates",
    "item['utterances'].append(text)",
    "break",
    "",
    "# Save enhanced corpus",
    "enhanced_corpus_path = 'app/chatbot/data/training/base-corpus_enhanced.json'",
    "with open(enhanced_corpus_path, 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "print(f\"\\n Enhanced corpus saved to: {enhanced_corpus_path}\")",
    "",
    "# Reload data and check improvement",
    "if enhanced_data:",
    "# Update the main corpus file for immediate use",
    "with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:",
    "json.dump(base_corpus, f, indent=2, ensure_ascii=False)",
    "",
    "# Reload and check new distribution",
    "texts_enhanced, intents_enhanced = load_corpus()",
    "df_enhanced = pd.DataFrame({'text': texts_enhanced, 'intent': intents_enhanced})",
    "",
    "enhanced_counts = df_enhanced['intent'].value_counts()",
    "",
    "print(f\"\\n BEFORE vs AFTER Enhancement:\")",
    "print(\"Intent\".ljust(25) + \"Before\".ljust(10) + \"After\".ljust(10) + \"Change\")",
    "print(\"-\" * 50)",
    "",
    "for intent in minority_classes:",
    "before = class_counts.get(intent, 0)",
    "after = enhanced_counts.get(intent, 0)",
    "change = after - before",
    "print(f\"{intent[:24].ljust(25)}{str(before).ljust(10)}{str(after).ljust(10)}+{change}\")",
    "",
    "# Calculate new imbalance ratio",
    "new_ratio = enhanced_counts.iloc[0] / enhanced_counts.iloc[-1]",
    "old_ratio = class_counts.iloc[0] / class_counts.iloc[-1]",
    "",
    "print(f\"\\n Class imbalance ratio:\")",
    "print(f\" Before: {old_ratio:.1f}:1\")",
    "print(f\" After: {new_ratio:.1f}:1\")",
    "print(f\" Improvement: {((old_ratio - new_ratio) / old_ratio * 100):.1f}% reduction\")",
    "",
    "return df_enhanced, len(enhanced_data)",
    "else:",
    "print(\" No minority classes found that need enhancement\")",
    "return df_final, 0",
    "",
    "# Run enhancement",
    "df_enhanced, num_added = analyze_and_enhance_minority_classes()",
    "",
    "print(f\"\\n Minority class enhancement complete\")",
    "print(f\" Added {num_added} new training examples\")",
    "print(f\" Total samples: {len(df_enhanced)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07a60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FINAL MODEL EVALUATION WITH ENHANCED DATA",
    "print(\"=== FINAL MODEL EVALUATION WITH ENHANCED DATA ===\")",
    "",
    "if num_added > 0:",
    "print(\"Training final model with enhanced dataset...\")",
    "",
    "# Train final model with enhanced data",
    "X_enhanced = df_enhanced['text']",
    "y_enhanced = df_enhanced['intent']",
    "",
    "# Create final pipeline",
    "final_pipeline = Pipeline([",
    "('tfidf', TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3), stop_words='english')),",
    "('clf', LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial', class_weight='balanced'))",
    "])",
    "",
    "# Train on enhanced data",
    "final_pipeline.fit(X_enhanced, y_enhanced)",
    "",
    "# Cross-validate enhanced model",
    "print(\"\\nRunning cross-validation on enhanced model...\")",
    "enhanced_cv_results = {}",
    "",
    "for metric_name, scorer in scoring.items():",
    "scores = cross_val_score(",
    "final_pipeline, X_enhanced, y_enhanced,",
    "cv=skf, scoring=scorer, n_jobs=-1",
    ")",
    "enhanced_cv_results[metric_name] = scores",
    "",
    "# Compare original vs enhanced model",
    "print(f\"\\n MODEL COMPARISON: Original vs Enhanced\")",
    "print(\"Metric\".ljust(15) + \"Original\".ljust(15) + \"Enhanced\".ljust(15) + \"Improvement\")",
    "print(\"-\" * 60)",
    "",
    "for metric in ['accuracy', 'macro_f1', 'weighted_f1']:",
    "orig_score = cv_results[metric].mean()",
    "enh_score = enhanced_cv_results[metric].mean()",
    "improvement = ((enh_score - orig_score) / orig_score) * 100",
    "",
    "print(f\"{metric.ljust(15)}{orig_score:.4f}\".ljust(15) + f\"{enh_score:.4f}\".ljust(15) + f\"{improvement:+.2f}%\")",
    "",
    "# Save final enhanced model",
    "final_model_path = 'app/models/logistic_regression_classifier_final_enhanced.joblib'",
    "joblib.dump(final_pipeline, final_model_path)",
    "",
    "# Create final metadata",
    "final_metadata = {",
    "\"model_type\": \"LogisticRegression_Final_Enhanced\",",
    "\"created_at\": datetime.now().isoformat(),",
    "\"total_samples\": len(df_enhanced),",
    "\"minority_samples_added\": num_added,",
    "\"unique_intents\": df_enhanced['intent'].nunique(),",
    "\"cv_performance\": {",
    "\"accuracy\": f\"{enhanced_cv_results['accuracy'].mean():.4f} ± {enhanced_cv_results['accuracy'].std():.4f}\",",
    "\"macro_f1\": f\"{enhanced_cv_results['macro_f1'].mean():.4f} ± {enhanced_cv_results['macro_f1'].std():.4f}\",",
    "\"weighted_f1\": f\"{enhanced_cv_results['weighted_f1'].mean():.4f} ± {enhanced_cv_results['weighted_f1'].std():.4f}\"",
    "},",
    "\"improvements_made\": [",
    "\"Fixed multi_class deprecation warning\",",
    "\"Removed break statements for multiple intent handling\",",
    "\"Added class imbalance handling with balanced weights\",",
    "\"Enhanced minority classes with additional examples\",",
    "\"Added comprehensive cross-validation\",",
    "\"Added confusion matrix analysis\",",
    "\"Added data versioning and backup system\"",
    "]",
    "}",
    "",
    "# Save final metadata",
    "with open('app/models/final_model_metadata.json', 'w') as f:",
    "json.dump(final_metadata, f, indent=2)",
    "",
    "print(f\"\\n Final enhanced model saved: {final_model_path}\")",
    "print(f\" Final metadata saved: app/models/final_model_metadata.json\")",
    "print(f\" Performance improvement achieved!\")",
    "",
    "# Test final model on original test cases",
    "test_cases = [",
    "'are you a bot',",
    "'bye for now',",
    "'Hello',",
    "'Who are the members of the band?',",
    "'Tell me about quantum physics'",
    "]",
    "",
    "print(f\"\\n FINAL MODEL TEST:\")",
    "final_predictions = final_pipeline.predict(test_cases)",
    "final_probabilities = final_pipeline.predict_proba(test_cases)",
    "",
    "for i, (query, pred) in enumerate(zip(test_cases, final_predictions)):",
    "confidence = np.max(final_probabilities[i])",
    "print(f\"'{query}' -> '{pred}' (confidence: {confidence:.3f})\")",
    "",
    "else:",
    "print(\"No enhancements were made - using original improved model as final model\")",
    "final_pipeline = pipeline_improved",
    "final_model_path = 'app/models/logistic_regression_classifier_fixed.joblib'",
    "",
    "print(f\"\\n === COMPREHENSIVE NOTEBOOK ANALYSIS COMPLETE ===\")",
    "print(f\" Confusion matrix analysis\")",
    "print(f\" 5-fold cross-validation\")",
    "print(f\" Data versioning and persistence\")",
    "print(f\" Minority class enhancement\")",
    "print(f\" Final model evaluation\")",
    "print(f\"\\n Ready for production use!\")",
    "print(f\" Use model: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac8b58-8705-4ef9-b41a-41856d6b2473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}