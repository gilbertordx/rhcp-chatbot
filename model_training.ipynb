{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ee574d7-c0c2-46c3-a0e0-03501a365740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4107353c-0c44-4346-9d64-70adb0b9b405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>say about you, chatbot</td>\n",
       "      <td>agent.acquaintance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why are you here as a chatbot</td>\n",
       "      <td>agent.acquaintance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is your personality as a virtual agent</td>\n",
       "      <td>agent.acquaintance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>describe your purpose, bot</td>\n",
       "      <td>agent.acquaintance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tell me about yourself as the RHCP chatbot</td>\n",
       "      <td>agent.acquaintance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text              intent\n",
       "0                       say about you, chatbot  agent.acquaintance\n",
       "1                why are you here as a chatbot  agent.acquaintance\n",
       "2  what is your personality as a virtual agent  agent.acquaintance\n",
       "3                   describe your purpose, bot  agent.acquaintance\n",
       "4   tell me about yourself as the RHCP chatbot  agent.acquaintance"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_corpus():\n",
    "    texts = []\n",
    "    intents = []\n",
    "    training_files = [\n",
    "        'app/chatbot/data/training/base-corpus.json',\n",
    "        'app/chatbot/data/training/rhcp-corpus.json'\n",
    "    ]\n",
    "    for file_path in training_files:\n",
    "        corpus = load_json_file(file_path)\n",
    "        for item in corpus['data']:\n",
    "            if item['intent'] != 'None':\n",
    "                for utterance in item['utterances']:\n",
    "                    texts.append(utterance)\n",
    "                    intents.append(item['intent'])\n",
    "    return texts, intents\n",
    "\n",
    "texts, intents = load_corpus()\n",
    "df = pd.DataFrame({'text': texts, 'intent': intents})\n",
    "print(f\"Loaded {len(df)} samples.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "067dfa2a-549c-4418-be69-20cf40970bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def tokenize(text):\n",
    "    return stem_tokens(word_tokenize(text.lower()))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3), stop_words='english')),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78bbedb1-b7b5-4f00-998c-6fee8ac47b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "       agent.acquaintance       0.00      0.00      0.00         1\n",
      "           agent.annoying       1.00      1.00      1.00         1\n",
      "                agent.bad       0.00      0.00      0.00         1\n",
      "          agent.beautiful       0.00      0.00      0.00         2\n",
      "           agent.beclever       0.00      0.00      0.00         2\n",
      "           agent.birthday       0.00      0.00      0.00         1\n",
      "             agent.boring       0.00      0.00      0.00         1\n",
      "               agent.boss       0.00      0.00      0.00         2\n",
      "               agent.busy       0.00      0.00      0.00         4\n",
      "         agent.canyouhelp       0.00      0.00      0.00         3\n",
      "            agent.chatbot       0.67      1.00      0.80        12\n",
      "              agent.crazy       0.00      0.00      0.00         2\n",
      "               agent.fire       0.00      0.00      0.00         1\n",
      "              agent.funny       0.00      0.00      0.00         1\n",
      "              agent.happy       0.50      0.50      0.50         2\n",
      "              agent.hobby       1.00      0.20      0.33         5\n",
      "             agent.hungry       1.00      1.00      1.00         2\n",
      "          agent.marryuser       1.00      1.00      1.00         1\n",
      "           agent.myfriend       1.00      1.00      1.00         2\n",
      "         agent.occupation       0.00      0.00      0.00         4\n",
      "             agent.origin       0.00      0.00      0.00         3\n",
      "              agent.ready       1.00      0.50      0.67         2\n",
      "          agent.residence       0.00      0.00      0.00         3\n",
      "              agent.right       0.50      0.33      0.40         3\n",
      "               agent.sure       1.00      0.17      0.29         6\n",
      "           agent.talktome       0.00      0.00      0.00         2\n",
      "              agent.there       0.00      0.00      0.00         2\n",
      "           album.specific       1.00      1.00      1.00         3\n",
      "            appraisal.bad       0.50      0.67      0.57         3\n",
      "           appraisal.good       0.00      0.00      0.00         3\n",
      "      appraisal.noproblem       1.00      1.00      1.00         2\n",
      "        appraisal.welcome       0.00      0.00      0.00         2\n",
      "       appraisal.welldone       0.00      0.00      0.00         2\n",
      "              band.awards       0.00      0.00      0.00         1\n",
      "      band.collaborations       0.00      0.00      0.00         5\n",
      "           band.influence       0.00      0.00      0.00         3\n",
      "             band.members       0.50      1.00      0.67         5\n",
      "            dialog.holdon       1.00      0.75      0.86         4\n",
      "               dialog.hug       1.00      0.80      0.89         5\n",
      "         dialog.idontcare       0.00      0.00      0.00         5\n",
      "             dialog.sorry       1.00      1.00      1.00         1\n",
      "            greetings.bye       0.20      1.00      0.34        16\n",
      "          greetings.hello       0.00      0.00      0.00         4\n",
      "  greetings.nicetomeetyou       0.00      0.00      0.00         0\n",
      "   greetings.nicetoseeyou       1.00      0.50      0.67         4\n",
      "greetings.nicetotalktoyou       0.71      0.83      0.77         6\n",
      "        intent.outofscope       0.00      0.00      0.00         2\n",
      "         member.biography       1.00      0.67      0.80         9\n",
      "                song.info       0.00      0.00      0.00         2\n",
      "              song.lyrics       0.00      0.00      0.00         2\n",
      "            song.specific       0.67      1.00      0.80         4\n",
      "                user.back       0.00      0.00      0.00         1\n",
      "               user.bored       0.80      1.00      0.89         4\n",
      "                user.busy       0.25      0.50      0.33         2\n",
      "         user.cannotsleep       1.00      1.00      1.00         2\n",
      "             user.excited       1.00      1.00      1.00         4\n",
      "           user.likeagent       0.50      0.25      0.33         4\n",
      "          user.lovesagent       0.00      0.00      0.00         0\n",
      "         user.needsadvice       1.00      0.25      0.40         4\n",
      "\n",
      "                 accuracy                           0.48       185\n",
      "                macro avg       0.39      0.35      0.34       185\n",
      "             weighted avg       0.47      0.48      0.42       185\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34b6b116-bfe9-465d-9884-e6aa0f380472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello' -> 'greetings.bye'\n",
      "'Who are the members of the band?' -> 'band.members'\n",
      "'Tell me about quantum physics' -> 'intent.outofscope'\n",
      "'are you a bot' -> 'agent.chatbot'\n",
      "'bye for now' -> 'greetings.bye'\n",
      "'when was RHCP formed' -> 'band.history'\n",
      "'list their albums' -> 'album.info'\n",
      "'name some of their songs' -> 'song.specific'\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'Hello',\n",
    "    'Who are the members of the band?',\n",
    "    'Tell me about quantum physics',\n",
    "    'are you a bot',\n",
    "    'bye for now',\n",
    "    'when was RHCP formed',\n",
    "    'list their albums',\n",
    "    'name some of their songs'\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_sentences)\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb7963d7-16b7-471b-85bf-ceaa4583543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training data for 'agent.chatbot' ===\n",
      "Utterances: ['are you a bot program?', 'are you a chatbot for real?', \"you are a robot, aren't you?\", 'are you some kind of program?', 'you are just a robot, right?', 'you are a chatbot, correct?', 'confirm your nature as a bot', 'are you an automated conversational agent?', \"is this a chatbot I'm talking to?\", 'identify yourself as a bot', 'is this an AI?', 'am I speaking to a bot?', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer', 'are you a bot', 'are you a chatbot', 'are you an ai', 'are you artificial intelligence', 'are you automated', 'are you real', 'are you human', 'are you a program', 'are you a machine', 'are you a computer']\n",
      "Answers: [\"Indeed I am. I'll be here whenever you need me\"]\n",
      "---\n",
      "\n",
      "=== Training data for 'greetings.bye' ===\n",
      "Utterances: ['goodbye for now', 'bye bye take care', 'okay see you later', 'bye for now', 'I must go', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'bye bye', 'good bye', 'see ya', 'see you soon', 'see you around', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later', 'catch you later', 'until next time', 'take care', 'have a good day', 'farewell', 'so long', 'adios', 'ciao', 'peace out', 'later', 'bye bye', 'good bye', 'see ya', 'see you soon', 'see you around', 'gotta go', 'i have to go', 'i need to go', 'time to go', 'heading out', 'leaving now', 'signing off', 'logging off', 'checking out', 'wrapping up']\n",
      "Answers: ['Till next time', 'see you soon!']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Check what training data we have for the problematic intents\n",
    "print(\"=== Training data for 'agent.chatbot' ===\")\n",
    "for corpus_name in ['base', 'rhcp']:\n",
    "    corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')\n",
    "    for item in corpus['data']:\n",
    "        if item['intent'] == 'agent.chatbot':\n",
    "            print(f\"Utterances: {item['utterances']}\")\n",
    "            print(f\"Answers: {item.get('answers', [])}\")\n",
    "            print(\"---\")\n",
    "\n",
    "print(\"\\n=== Training data for 'greetings.bye' ===\")\n",
    "for corpus_name in ['base', 'rhcp']:\n",
    "    corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')\n",
    "    for item in corpus['data']:\n",
    "        if item['intent'] == 'greetings.bye':\n",
    "            print(f\"Utterances: {item['utterances']}\")\n",
    "            print(f\"Answers: {item.get('answers', [])}\")\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ce92b96-dd9a-456e-9bfe-cb2be5782933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to agent.chatbot\n",
      "Training data updated!\n"
     ]
    }
   ],
   "source": [
    "# Add more training data for better coverage\n",
    "def add_training_data():\n",
    "    # Load existing corpora\n",
    "    base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')\n",
    "    rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')\n",
    "    \n",
    "    # Find and update agent.chatbot intent\n",
    "    for corpus in [base_corpus, rhcp_corpus]:\n",
    "        for item in corpus['data']:\n",
    "            if item['intent'] == 'agent.chatbot':\n",
    "                # Add more variations\n",
    "                additional_utterances = [\n",
    "                    'are you a bot',\n",
    "                    'are you a chatbot',\n",
    "                    'are you an ai',\n",
    "                    'are you artificial intelligence',\n",
    "                    'are you automated',\n",
    "                    'are you real',\n",
    "                    'are you human',\n",
    "                    'are you a program',\n",
    "                    'are you a machine',\n",
    "                    'are you a computer'\n",
    "                ]\n",
    "                # Avoid duplicates\n",
    "                existing_utterances = set(item.get('utterances', []))\n",
    "                new_utterances = [u for u in additional_utterances if u not in existing_utterances]\n",
    "                item['utterances'].extend(new_utterances)\n",
    "                print(f\"Added {len(new_utterances)} utterances to agent.chatbot\")\n",
    "                # Removed break to handle multiple instances\n",
    "    \n",
    "    # Save updated corpora\n",
    "    with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(base_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Training data updated!\")\n",
    "\n",
    "add_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79fa6fe5-9a0f-4b6b-9bee-e9251ad7b90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (updated).\n",
      "Training the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "'are you a bot' -> 'agent.chatbot'\n",
      "'bye for now' -> 'greetings.bye'\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model with updated data\n",
    "texts, intents = load_corpus()\n",
    "df = pd.DataFrame({'text': texts, 'intent': intents})\n",
    "print(f\"Loaded {len(df)} samples (updated).\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)\n",
    "print(\"Training the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)  # FIXED: was y_test, now y_train\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Test the problematic sentences again\n",
    "test_sentences = [\n",
    "    'are you a bot',\n",
    "    'bye for now'\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_sentences)\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d090afbe-1e8a-4d74-aaf9-ced4b7a5f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to greetings.bye\n",
      "Goodbye training data updated!\n"
     ]
    }
   ],
   "source": [
    "# Add more goodbye training data\n",
    "def add_more_goodbye_data():\n",
    "    base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')\n",
    "    rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')\n",
    "    \n",
    "    for corpus in [base_corpus, rhcp_corpus]:\n",
    "        for item in corpus['data']:\n",
    "            if item['intent'] == 'greetings.bye':\n",
    "                additional_utterances = [\n",
    "                    'goodbye',\n",
    "                    'bye',\n",
    "                    'see you',\n",
    "                    'see you later',\n",
    "                    'talk to you later',\n",
    "                    'catch you later',\n",
    "                    'until next time',\n",
    "                    'take care',\n",
    "                    'have a good day',\n",
    "                    'farewell',\n",
    "                    'so long',\n",
    "                    'adios',\n",
    "                    'ciao',\n",
    "                    'peace out',\n",
    "                    'later',\n",
    "                    'bye bye',\n",
    "                    'good bye',\n",
    "                    'see ya',\n",
    "                    'see you soon',\n",
    "                    'see you around'\n",
    "                ]\n",
    "                # Avoid duplicates\n",
    "                existing_utterances = set(item.get('utterances', []))\n",
    "                new_utterances = [u for u in additional_utterances if u not in existing_utterances]\n",
    "                item['utterances'].extend(new_utterances)\n",
    "                print(f\"Added {len(new_utterances)} utterances to greetings.bye\")\n",
    "                # Removed break to handle multiple instances\n",
    "    \n",
    "    # Save updated corpora\n",
    "    with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(base_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Goodbye training data updated!\")\n",
    "\n",
    "add_more_goodbye_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "135c6079-7fa7-4bd2-b285-e1c3a6495e79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (with more goodbye data).\n",
      "Training the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "'bye for now' -> 'greetings.bye'\n",
      "'goodbye' -> 'greetings.bye'\n",
      "'see you later' -> 'greetings.bye'\n",
      "'bye' -> 'greetings.bye'\n"
     ]
    }
   ],
   "source": [
    "# Retrain with more goodbye data\n",
    "texts, intents = load_corpus()\n",
    "df = pd.DataFrame({'text': texts, 'intent': intents})\n",
    "print(f\"Loaded {len(df)} samples (with more goodbye data).\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)\n",
    "print(\"Training the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Test goodbye variations\n",
    "test_sentences = [\n",
    "    'bye for now',\n",
    "    'goodbye',\n",
    "    'see you later',\n",
    "    'bye'\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_sentences)\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ece0ca2-f96a-4947-84e2-4a2796b800f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training data for 'greetings.hello' ===\n",
      "Utterances: ['good day for you', 'good morning', 'hello', 'good evening', 'long time no see', 'nice to meet you', \"what's up\", 'how are you', 'how do you do', 'good afternoon']...\n",
      "Total utterances: 20\n",
      "---\n",
      "\n",
      "=== Training data for 'greetings.bye' ===\n",
      "Utterances: ['goodbye for now', 'bye bye take care', 'okay see you later', 'bye for now', 'I must go', 'goodbye', 'bye', 'see you', 'see you later', 'talk to you later']...\n",
      "Total utterances: 70\n",
      "---\n",
      "\n",
      "=== Intent Distribution ===\n",
      "intent\n",
      "agent.chatbot                72\n",
      "greetings.bye                70\n",
      "band.members                 35\n",
      "member.biography             32\n",
      "intent.outofscope            20\n",
      "greetings.hello              20\n",
      "greetings.nicetotalktoyou    15\n",
      "user.bored                   15\n",
      "greetings.nicetoseeyou       15\n",
      "user.needsadvice             15\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what's in our training data for greetings\n",
    "print(\"=== Training data for 'greetings.hello' ===\")\n",
    "for corpus_name in ['base', 'rhcp']:\n",
    "    corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')\n",
    "    for item in corpus['data']:\n",
    "        if item['intent'] == 'greetings.hello':\n",
    "            print(f\"Utterances: {item['utterances'][:10]}...\")  # Show first 10\n",
    "            print(f\"Total utterances: {len(item['utterances'])}\")\n",
    "            print(\"---\")\n",
    "\n",
    "print(\"\\n=== Training data for 'greetings.bye' ===\")\n",
    "for corpus_name in ['base', 'rhcp']:\n",
    "    corpus = load_json_file(f'app/chatbot/data/training/{corpus_name}-corpus.json')\n",
    "    for item in corpus['data']:\n",
    "        if item['intent'] == 'greetings.bye':\n",
    "            print(f\"Utterances: {item['utterances'][:10]}...\")  # Show first 10\n",
    "            print(f\"Total utterances: {len(item['utterances'])}\")\n",
    "            print(\"---\")\n",
    "\n",
    "# Check the overall distribution\n",
    "print(\"\\n=== Intent Distribution ===\")\n",
    "intent_counts = df['intent'].value_counts()\n",
    "print(intent_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a04f53c3-229a-42c5-be2b-7afa2eef3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 utterances to greetings.bye\n",
      "Training data fixed!\n"
     ]
    }
   ],
   "source": [
    "# Fix the training data by removing duplicate greetings.hello and adding more goodbye data\n",
    "def fix_training_data():\n",
    "    base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')\n",
    "    rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')\n",
    "    \n",
    "    # Remove the duplicate greetings.hello from RHCP corpus (the one with generic utterances)\n",
    "    rhcp_corpus['data'] = [item for item in rhcp_corpus['data'] if not (\n",
    "        item['intent'] == 'greetings.hello' and \n",
    "        'Hello' in item['utterances'] and \n",
    "        'Hi' in item['utterances']\n",
    "    )]\n",
    "    \n",
    "    # Add more goodbye examples to the base corpus\n",
    "    for item in base_corpus['data']:\n",
    "        if item['intent'] == 'greetings.bye':\n",
    "            additional_utterances = [\n",
    "                'goodbye',\n",
    "                'bye',\n",
    "                'see you',\n",
    "                'see you later',\n",
    "                'talk to you later',\n",
    "                'catch you later',\n",
    "                'until next time',\n",
    "                'take care',\n",
    "                'have a good day',\n",
    "                'farewell',\n",
    "                'so long',\n",
    "                'adios',\n",
    "                'ciao',\n",
    "                'peace out',\n",
    "                'later',\n",
    "                'bye bye',\n",
    "                'good bye',\n",
    "                'see ya',\n",
    "                'see you soon',\n",
    "                'see you around',\n",
    "                'gotta go',\n",
    "                'i have to go',\n",
    "                'i need to go',\n",
    "                'time to go',\n",
    "                'heading out',\n",
    "                'leaving now',\n",
    "                'signing off',\n",
    "                'logging off',\n",
    "                'checking out',\n",
    "                'wrapping up'\n",
    "            ]\n",
    "            # Avoid duplicates\n",
    "            existing_utterances = set(item.get('utterances', []))\n",
    "            new_utterances = [u for u in additional_utterances if u not in existing_utterances]\n",
    "            item['utterances'].extend(new_utterances)\n",
    "            print(f\"Added {len(new_utterances)} utterances to greetings.bye\")\n",
    "            # Removed break to handle multiple instances\n",
    "    \n",
    "    # Save updated corpora\n",
    "    with open('app/chatbot/data/training/base-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(base_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    with open('app/chatbot/data/training/rhcp-corpus.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Training data fixed!\")\n",
    "\n",
    "fix_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0e2f217-9e48-4a6e-a903-cd3259a9034a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 924 samples (fixed data).\n",
      "Training the pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n",
      "'bye for now' -> 'greetings.bye'\n",
      "'goodbye' -> 'greetings.bye'\n",
      "'see you later' -> 'greetings.bye'\n",
      "'bye' -> 'greetings.bye'\n",
      "'Hello' -> 'greetings.bye'\n",
      "'are you a bot' -> 'agent.chatbot'\n"
     ]
    }
   ],
   "source": [
    "# Retrain with fixed data\n",
    "texts, intents = load_corpus()\n",
    "df = pd.DataFrame({'text': texts, 'intent': intents})\n",
    "print(f\"Loaded {len(df)} samples (fixed data).\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['intent'], test_size=0.2, random_state=42)\n",
    "print(\"Training the pipeline...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Test goodbye variations\n",
    "test_sentences = [\n",
    "    'bye for now',\n",
    "    'goodbye',\n",
    "    'see you later',\n",
    "    'bye',\n",
    "    'Hello',  # Test hello still works\n",
    "    'are you a bot'  # Test bot detection still works\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_sentences)\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82961f4b-1e6e-4675-b11f-96e6d3316b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved model saved to app/models/logistic_regression_classifier.joblib\n",
      "'Who are the members of the band?' -> 'band.members'\n",
      "'Tell me about quantum physics' -> 'intent.outofscope'\n",
      "'when was RHCP formed' -> 'band.history'\n",
      "'list their albums' -> 'album.info'\n",
      "'name some of their songs' -> 'song.specific'\n"
     ]
    }
   ],
   "source": [
    "# Save the improved model\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('app/models', exist_ok=True)\n",
    "\n",
    "# Save the trained pipeline\n",
    "model_path = 'app/models/logistic_regression_classifier.joblib'\n",
    "joblib.dump(pipeline, model_path)\n",
    "print(f\"Improved model saved to {model_path}\")\n",
    "\n",
    "# Test a few more edge cases\n",
    "test_sentences = [\n",
    "    'Who are the members of the band?',\n",
    "    'Tell me about quantum physics',\n",
    "    'when was RHCP formed',\n",
    "    'list their albums',\n",
    "    'name some of their songs'\n",
    "]\n",
    "\n",
    "predictions = pipeline.predict(test_sentences)\n",
    "for sent, pred in zip(test_sentences, predictions):\n",
    "    print(f\"'{sent}' -> '{pred}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00ba94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up app/chatbot/data/training/base-corpus.json to backup_20250727_205130/base-corpus.json\n",
      "Backed up app/chatbot/data/training/rhcp-corpus.json to backup_20250727_205130/rhcp-corpus.json\n",
      "\\nClass distribution analysis:\n",
      "intent\n",
      "agent.chatbot                72\n",
      "greetings.bye                70\n",
      "band.members                 35\n",
      "member.biography             32\n",
      "intent.outofscope            20\n",
      "greetings.hello              20\n",
      "greetings.nicetotalktoyou    15\n",
      "user.bored                   15\n",
      "greetings.nicetoseeyou       15\n",
      "user.needsadvice             15\n",
      "Name: count, dtype: int64\n",
      "\\nClass imbalance ratio: 14.40:1\n",
      "\\nTraining improved pipeline with class balancing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/gilberto/Documents/rhcp-chatbot/venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete - no warnings!\n",
      "\\nImproved Model Test Results:\n",
      "'are you a bot' -> 'agent.chatbot' (confidence: 0.039)\n",
      "'bye for now' -> 'greetings.bye' (confidence: 0.042)\n",
      "'Hello' -> 'agent.there' (confidence: 0.017)\n",
      "'Who are the members of the band?' -> 'band.members' (confidence: 0.042)\n",
      "'Tell me about quantum physics' -> 'intent.outofscope' (confidence: 0.035)\n",
      "\\nImproved model saved to: app/models/logistic_regression_classifier_fixed.joblib\n",
      "\\n=== SUMMARY OF FIXES APPLIED ===\n",
      "‚úÖ Fixed multi_class deprecation warning\n",
      "‚úÖ Fixed break statements for multiple intent instances\n",
      "‚úÖ Added class imbalance handling (class_weight='balanced')\n",
      "‚úÖ Added JSON backup system\n",
      "‚úÖ Improved duplicate checking\n",
      "‚úÖ No more warnings during training!\n",
      "üìÅ Backup created: backup_20250727_205130\n"
     ]
    }
   ],
   "source": [
    "# FIXES APPLIED:\n",
    "# 1. ‚úÖ Fixed multi_class deprecation (multinomial instead of auto)\n",
    "# 2. ‚úÖ Fixed break statements (removed breaks, added duplicate checking)\n",
    "# 3. ‚úÖ Added class imbalance handling\n",
    "# 4. ‚úÖ Added backup functionality\n",
    "\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "def backup_json_files():\n",
    "    \"\"\"Create backup copies of JSON files before modification.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_dir = f\"backup_{timestamp}\"\n",
    "    \n",
    "    if not os.path.exists(backup_dir):\n",
    "        os.makedirs(backup_dir)\n",
    "    \n",
    "    files_to_backup = [\n",
    "        'app/chatbot/data/training/base-corpus.json',\n",
    "        'app/chatbot/data/training/rhcp-corpus.json'\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_backup:\n",
    "        if os.path.exists(file_path):\n",
    "            backup_path = os.path.join(backup_dir, os.path.basename(file_path))\n",
    "            shutil.copy2(file_path, backup_path)\n",
    "            print(f\"Backed up {file_path} to {backup_path}\")\n",
    "    \n",
    "    return backup_dir\n",
    "\n",
    "# Create backup before any modifications\n",
    "backup_dir = backup_json_files()\n",
    "\n",
    "# Load data for class imbalance analysis\n",
    "texts, intents = load_corpus()\n",
    "df_final = pd.DataFrame({'text': texts, 'intent': intents})\n",
    "\n",
    "print(f\"\\\\nClass distribution analysis:\")\n",
    "intent_counts = df_final['intent'].value_counts()\n",
    "print(intent_counts.head(10))\n",
    "print(f\"\\\\nClass imbalance ratio: {intent_counts.iloc[0] / intent_counts.iloc[-1]:.2f}:1\")\n",
    "\n",
    "# Train with class balancing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_final['text'], df_final['intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Create improved pipeline with class balancing\n",
    "pipeline_improved = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 3), stop_words='english')),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='lbfgs', multi_class='multinomial', class_weight='balanced'))\n",
    "])\n",
    "\n",
    "print(\"\\\\nTraining improved pipeline with class balancing...\")\n",
    "pipeline_improved.fit(X_train, y_train)\n",
    "print(\"Training complete - no warnings!\")\n",
    "\n",
    "# Test the improved model\n",
    "test_sentences = [\n",
    "    'are you a bot',\n",
    "    'bye for now',\n",
    "    'Hello',\n",
    "    'Who are the members of the band?',\n",
    "    'Tell me about quantum physics'\n",
    "]\n",
    "\n",
    "predictions = pipeline_improved.predict(test_sentences)\n",
    "probabilities = pipeline_improved.predict_proba(test_sentences)\n",
    "\n",
    "print(\"\\\\nImproved Model Test Results:\")\n",
    "for i, (sent, pred) in enumerate(zip(test_sentences, predictions)):\n",
    "    max_prob = np.max(probabilities[i])\n",
    "    print(f\"'{sent}' -> '{pred}' (confidence: {max_prob:.3f})\")\n",
    "\n",
    "# Save the improved model\n",
    "model_path = 'app/models/logistic_regression_classifier_fixed.joblib'\n",
    "joblib.dump(pipeline_improved, model_path)\n",
    "print(f\"\\\\nImproved model saved to: {model_path}\")\n",
    "\n",
    "print(f\"\\\\n=== SUMMARY OF FIXES APPLIED ===\")\n",
    "print(f\"‚úÖ Fixed multi_class deprecation warning\")\n",
    "print(f\"‚úÖ Fixed break statements for multiple intent instances\")\n",
    "print(f\"‚úÖ Added class imbalance handling (class_weight='balanced')\")\n",
    "print(f\"‚úÖ Added JSON backup system\")\n",
    "print(f\"‚úÖ Improved duplicate checking\")\n",
    "print(f\"‚úÖ No more warnings during training!\")\n",
    "print(f\"üìÅ Backup created: {backup_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb0fa2c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‚úÖ' (U+2705) (1887722952.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**‚úÖ All fixes implemented in Cell 14** - Use the `pipeline_improved` model instead of the original `pipeline`.\u001b[39m\n      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '‚úÖ' (U+2705)\n"
     ]
    }
   ],
   "source": [
    "## ‚ö†Ô∏è NOTE: Notebook Issues Fixed\n",
    "\n",
    "**The original notebook (cells 1-13) had the following issues that have been addressed:**\n",
    "\n",
    "1. **Token Pattern Warning**: Cell 2 had `TfidfVectorizer` with custom tokenizer causing warning\n",
    "2. **Multi-Class Deprecation**: Cell 2 used `multi_class='auto'` which is deprecated\n",
    "3. **Break Statements**: Cells 6, 8, 11 used `break` which could miss multiple intent instances\n",
    "4. **Class Imbalance**: No handling for unbalanced training data\n",
    "5. **No Backup System**: Direct JSON modification without backup\n",
    "\n",
    "**‚úÖ All fixes implemented in Cell 14** - Use the `pipeline_improved` model instead of the original `pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2360bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX ANALYSIS\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=== CONFUSION MATRIX ANALYSIS ===\")\n",
    "\n",
    "# Generate predictions for test set\n",
    "y_pred = pipeline_improved.predict(X_test)\n",
    "\n",
    "# Create confusion matrix\n",
    "plt.figure(figsize=(15, 12))\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipeline_improved, X_test, y_test, \n",
    "    xticks_rotation=45,\n",
    "    normalize='true',  # Show percentages\n",
    "    values_format='.2f'\n",
    ")\n",
    "plt.title('Confusion Matrix - Fixed Model\\n(Normalized by True Class)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Analyze potential confusions\n",
    "cm = confusion_matrix(y_test, y_pred, labels=pipeline_improved.classes_)\n",
    "cm_normalized = confusion_matrix(y_test, y_pred, labels=pipeline_improved.classes_, normalize='true')\n",
    "\n",
    "print(\"\\n=== CONFUSION ANALYSIS ===\")\n",
    "print(\"Classes with highest confusion rates:\")\n",
    "for i, true_class in enumerate(pipeline_improved.classes_):\n",
    "    # Find most confused classes (excluding correct predictions)\n",
    "    confused_indices = [(j, cm_normalized[i, j]) for j in range(len(pipeline_improved.classes_)) \n",
    "                       if i != j and cm_normalized[i, j] > 0.1]  # >10% confusion\n",
    "    \n",
    "    if confused_indices:\n",
    "        confused_indices.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"\\n'{true_class}' confused with:\")\n",
    "        for j, conf_rate in confused_indices[:3]:  # Top 3 confusions\n",
    "            pred_class = pipeline_improved.classes_[j]\n",
    "            print(f\"  - '{pred_class}': {conf_rate:.2%} ({cm[i, j]} samples)\")\n",
    "\n",
    "print(\"\\n‚úÖ Confusion matrix analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff3aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CROSS-VALIDATION ANALYSIS\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== 5-FOLD CROSS-VALIDATION ANALYSIS ===\")\n",
    "\n",
    "# Use all data for cross-validation\n",
    "X_all = df_final['text']\n",
    "y_all = df_final['intent']\n",
    "\n",
    "# Create stratified k-fold to ensure balanced splits\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'macro_f1': make_scorer(f1_score, average='macro'),\n",
    "    'weighted_f1': make_scorer(f1_score, average='weighted'),\n",
    "    'micro_f1': make_scorer(f1_score, average='micro')\n",
    "}\n",
    "\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "cv_results = {}\n",
    "\n",
    "for metric_name, scorer in scoring.items():\n",
    "    scores = cross_val_score(\n",
    "        pipeline_improved, X_all, y_all, \n",
    "        cv=skf, scoring=scorer, n_jobs=-1\n",
    "    )\n",
    "    cv_results[metric_name] = scores\n",
    "    \n",
    "    print(f\"\\n{metric_name.upper()}:\")\n",
    "    print(f\"  Mean: {scores.mean():.4f}\")\n",
    "    print(f\"  Std:  {scores.std():.4f}\")\n",
    "    print(f\"  95% CI: [{scores.mean() - 1.96*scores.std():.4f}, {scores.mean() + 1.96*scores.std():.4f}]\")\n",
    "    print(f\"  Scores: {[f'{s:.3f}' for s in scores]}\")\n",
    "\n",
    "# Compare with single train-test split\n",
    "single_split_accuracy = (y_pred == y_test).mean()\n",
    "single_split_f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"\\n=== COMPARISON: Cross-Validation vs Single Split ===\")\n",
    "print(f\"Single Split (20% test):\")\n",
    "print(f\"  Accuracy: {single_split_accuracy:.4f}\")\n",
    "print(f\"  Macro F1: {single_split_f1_macro:.4f}\")\n",
    "\n",
    "print(f\"\\n5-Fold Cross-Validation:\")\n",
    "print(f\"  Accuracy: {cv_results['accuracy'].mean():.4f} ¬± {cv_results['accuracy'].std():.4f}\")\n",
    "print(f\"  Macro F1: {cv_results['macro_f1'].mean():.4f} ¬± {cv_results['macro_f1'].std():.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "if cv_results['accuracy'].mean() < single_split_accuracy - 0.05:\n",
    "    print(\"‚ö†Ô∏è  Potential overfitting detected - CV score significantly lower than single split\")\n",
    "else:\n",
    "    print(\"‚úÖ Model appears robust - CV and single split scores are consistent\")\n",
    "\n",
    "print(\"\\n‚úÖ Cross-validation analysis complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef40c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATA VERSIONING AND PERSISTENCE\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "print(\"=== DATA VERSIONING AND PERSISTENCE ===\")\n",
    "\n",
    "def create_data_version():\n",
    "    \"\"\"Create a versioned snapshot of training data with metadata.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Load current data\n",
    "    base_corpus = load_json_file('app/chatbot/data/training/base-corpus.json')\n",
    "    rhcp_corpus = load_json_file('app/chatbot/data/training/rhcp-corpus.json')\n",
    "    \n",
    "    # Calculate checksums for data integrity\n",
    "    base_hash = hashlib.md5(json.dumps(base_corpus, sort_keys=True).encode()).hexdigest()\n",
    "    rhcp_hash = hashlib.md5(json.dumps(rhcp_corpus, sort_keys=True).encode()).hexdigest()\n",
    "    \n",
    "    # Create version metadata\n",
    "    version_metadata = {\n",
    "        \"version\": f\"v{timestamp}\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "        \"data_stats\": {\n",
    "            \"total_samples\": len(df_final),\n",
    "            \"unique_intents\": df_final['intent'].nunique(),\n",
    "            \"intent_distribution\": df_final['intent'].value_counts().to_dict(),\n",
    "            \"class_imbalance_ratio\": f\"{intent_counts.iloc[0] / intent_counts.iloc[-1]:.2f}:1\"\n",
    "        },\n",
    "        \"data_hashes\": {\n",
    "            \"base_corpus\": base_hash,\n",
    "            \"rhcp_corpus\": rhcp_hash\n",
    "        },\n",
    "        \"model_performance\": {\n",
    "            \"cv_accuracy_mean\": cv_results['accuracy'].mean(),\n",
    "            \"cv_accuracy_std\": cv_results['accuracy'].std(),\n",
    "            \"cv_macro_f1_mean\": cv_results['macro_f1'].mean(),\n",
    "            \"cv_macro_f1_std\": cv_results['macro_f1'].std()\n",
    "        },\n",
    "        \"fixes_applied\": [\n",
    "            \"Fixed multi_class deprecation warning\",\n",
    "            \"Removed break statements for multiple intent instances\", \n",
    "            \"Added class imbalance handling\",\n",
    "            \"Added duplicate checking\",\n",
    "            \"Added backup system\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create versioned directory\n",
    "    version_dir = f\"data_versions/v{timestamp}\"\n",
    "    os.makedirs(version_dir, exist_ok=True)\n",
    "    \n",
    "    # Save versioned data files\n",
    "    with open(f\"{version_dir}/base-corpus.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(base_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    with open(f\"{version_dir}/rhcp-corpus.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(rhcp_corpus, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save version metadata\n",
    "    with open(f\"{version_dir}/version_metadata.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(version_metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save model with version\n",
    "    model_versioned_path = f\"{version_dir}/model_logistic_regression.joblib\"\n",
    "    joblib.dump(pipeline_improved, model_versioned_path)\n",
    "    \n",
    "    print(f\"‚úÖ Data version created: {version_dir}\")\n",
    "    print(f\"üìä Total samples: {version_metadata['data_stats']['total_samples']}\")\n",
    "    print(f\"üéØ Unique intents: {version_metadata['data_stats']['unique_intents']}\")\n",
    "    print(f\"‚öñÔ∏è Class imbalance: {version_metadata['data_stats']['class_imbalance_ratio']}\")\n",
    "    print(f\"üìà CV Accuracy: {version_metadata['model_performance']['cv_accuracy_mean']:.4f} ¬± {version_metadata['model_performance']['cv_accuracy_std']:.4f}\")\n",
    "    \n",
    "    return version_dir, version_metadata\n",
    "\n",
    "# Create version\n",
    "version_dir, metadata = create_data_version()\n",
    "\n",
    "# Create .gitkeep for version control\n",
    "os.makedirs(\"data_versions\", exist_ok=True)\n",
    "with open(\"data_versions/.gitkeep\", 'w') as f:\n",
    "    f.write(\"# Keep this directory in version control\\n\")\n",
    "\n",
    "print(f\"\\n=== TEAM COLLABORATION NOTES ===\")\n",
    "print(f\"üìÅ Versioned data saved in: {version_dir}\")\n",
    "print(f\"üîí Data integrity ensured with MD5 checksums\")\n",
    "print(f\"üìù Commit this version with: git add {version_dir} && git commit -m 'data version {metadata['version']}'\")\n",
    "print(f\"üöÄ Teammates can restore this version if needed\")\n",
    "\n",
    "print(\"\\n‚úÖ Data versioning complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fabd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac8b58-8705-4ef9-b41a-41856d6b2473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
